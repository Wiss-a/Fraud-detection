{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fraud Detection - Data Preprocessing & Model Training\n",
        "## Azure Cloud Data-Driven Application Project\n",
        "\n",
        "**Project:** Detection of fraud in financial transactions  \n",
        "**Dataset:** 6M rows ‚Üí Truncated & Balanced for optimal training  \n",
        "**Goal:** Train a high-performance fraud detection model\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline Overview:\n",
        "1. **Data Loading & Exploration**\n",
        "2. **Strategic Truncation** (All frauds + 500k non-frauds)\n",
        "3. **Data Balancing with SMOTE** (40/60 or customizable ratio)\n",
        "4. **Feature Engineering & Preprocessing**\n",
        "5. **Model Training** (Multiple algorithms)\n",
        "6. **Model Evaluation & Comparison**\n",
        "7. **Export Best Model** for deployment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ 1. Import Required Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlflow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: mlflow in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (3.7.0)\nRequirement already satisfied: mlflow-tracing==3.7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.7.0)\nRequirement already satisfied: docker<8,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (7.1.0)\nRequirement already satisfied: cryptography<47,>=43.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (46.0.3)\nRequirement already satisfied: gunicorn<24 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (23.0.0)\nRequirement already satisfied: huey<3,>=2.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (2.5.5)\nRequirement already satisfied: numpy<3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (2.2.6)\nRequirement already satisfied: mlflow-skinny==3.7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.7.0)\nRequirement already satisfied: pyarrow<23,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (14.0.2)\nRequirement already satisfied: scikit-learn<2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.7.2)\nRequirement already satisfied: graphene<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: pandas<3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.5.3)\nRequirement already satisfied: Flask-CORS<7 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (5.0.1)\nRequirement already satisfied: alembic!=1.10.0,<2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.17.2)\nRequirement already satisfied: Flask<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.0.3)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (2.0.45)\nRequirement already satisfied: scipy<2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (1.15.3)\nRequirement already satisfied: matplotlib<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow) (3.7.1)\nRequirement already satisfied: cloudpickle<4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (2.2.1)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (6.33.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (2.32.4)\nRequirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (1.35.0)\nRequirement already satisfied: python-dotenv<2,>=0.19.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (1.1.0)\nRequirement already satisfied: fastapi<1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (0.115.12)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (1.35.0)\nRequirement already satisfied: pyyaml<7,>=5.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (6.0.2)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (0.49.0)\nRequirement already satisfied: packaging<26 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (25.0)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (5.5.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (0.5.3)\nRequirement already satisfied: click<9,>=7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (8.1.8)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (4.14.1)\nRequirement already satisfied: uvicorn<1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (0.34.0)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (8.2.0)\nRequirement already satisfied: pydantic<3,>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (2.9.2)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (3.1.44)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from mlflow-skinny==3.7.0->mlflow) (1.35.0)\nRequirement already satisfied: tomli in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (2.2.1)\nRequirement already satisfied: Mako in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\nRequirement already satisfied: cffi>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\nRequirement already satisfied: urllib3>=1.26.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\nRequirement already satisfied: itsdangerous>=2.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Flask<4->mlflow) (2.1.2)\nRequirement already satisfied: blinker>=1.6.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: Werkzeug>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: Jinja2>=3.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Flask<4->mlflow) (3.1.6)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from graphene<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.7)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.2.3)\nRequirement already satisfied: pillow>=6.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.2.0)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.51.0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pandas<3->mlflow) (2022.5)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.6.0)\nRequirement already satisfied: greenlet>=1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow) (2.22)\nRequirement already satisfied: google-auth~=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (2.38.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from fastapi<1->mlflow-skinny==3.7.0->mlflow) (0.46.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.7.0->mlflow) (4.0.12)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.7.0->mlflow) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask<4->mlflow) (3.0.3)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.7.0->mlflow) (0.56b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.7.0->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.7.0->mlflow) (2.23.4)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (2025.7.9)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (3.10)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (3.4.2)\nRequirement already satisfied: h11>=0.8 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from uvicorn<1->mlflow-skinny==3.7.0->mlflow) (0.16.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.7.0->mlflow) (5.0.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (0.4.2)\nRequirement already satisfied: anyio<5,>=3.6.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.7.0->mlflow) (4.9.0)\nRequirement already satisfied: sniffio>=1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.7.0->mlflow) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.7.0->mlflow) (1.3.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (0.6.1)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765909918135
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install xgboost lightgbm\n",
        "%pip install imbalanced-learn\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: xgboost in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.5.2)\nRequirement already satisfied: lightgbm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (4.6.0)\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (1.15.3)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from xgboost) (2.2.6)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: imbalanced-learn in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (0.14.0)\nRequirement already satisfied: scipy<2,>=1.11.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (1.15.3)\nRequirement already satisfied: threadpoolctl<4,>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (3.6.0)\nRequirement already satisfied: joblib<2,>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (1.2.0)\nRequirement already satisfied: scikit-learn<2,>=1.4.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (1.7.2)\nRequirement already satisfied: numpy<3,>=1.25.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (2.2.6)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Reinstall compatible numpy\n",
        "!pip install --upgrade --force-reinstall numpy==1.26.4\n",
        "\n",
        "# Step 2: Reinstall dependent packages to avoid binary incompatibility\n",
        "!pip install --upgrade --force-reinstall pandas==2.1.1 scipy==1.10.1 scikit-learn==1.2.2 imbalanced-learn==0.14.0\n",
        "\n",
        "# Step 3: Restart kernel after installation\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.2.6\n    Uninstalling numpy-2.2.6:\n      Successfully uninstalled numpy-2.2.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-ml 0.6.1 requires enum34, which is not installed.\ntensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\nscikit-image 0.25.2 requires pillow>=10.1, but you have pillow 9.2.0 which is incompatible.\nresponsibleai 0.36.0 requires networkx<=2.5, but you have networkx 3.4.2 which is incompatible.\nresponsibleai 0.36.0 requires numpy<=1.26.2,>=1.17.2, but you have numpy 1.26.4 which is incompatible.\nresponsibleai 0.36.0 requires scikit-learn!=1.1,<=1.5.1,>=0.22.1, but you have scikit-learn 1.7.2 which is incompatible.\nresponsibleai 0.36.0 requires semver~=2.13.0, but you have semver 3.0.4 which is incompatible.\nraiwidgets 0.36.0 requires numpy<=1.26.2,>=1.17.2, but you have numpy 1.26.4 which is incompatible.\nnumba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.4 which is incompatible.\neconml 0.15.1 requires scikit-learn<1.6,>=1.0, but you have scikit-learn 1.7.2 which is incompatible.\ndask-sql 2024.5.0 requires dask[dataframe]>=2024.4.1, but you have dask 2023.2.0 which is incompatible.\ndask-sql 2024.5.0 requires distributed>=2024.4.1, but you have distributed 2023.2.0 which is incompatible.\nazureml-training-tabular 1.60.0 requires numpy<=1.23.5,>=1.16.0; python_version >= \"3.8\", but you have numpy 1.26.4 which is incompatible.\nazureml-training-tabular 1.60.0 requires scikit-learn<=1.6,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\nazureml-training-tabular 1.60.0 requires scipy<1.11.0,>=1.0.0, but you have scipy 1.15.3 which is incompatible.\nazureml-training-tabular 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\nazureml-train-automl-runtime 1.60.0 requires azureml-dataset-runtime[fuse,pandas]~=1.60.0, but you have azureml-dataset-runtime 1.61.0 which is incompatible.\nazureml-train-automl-runtime 1.60.0 requires numpy<=1.23.5,>=1.16.0; python_version >= \"3.8\", but you have numpy 1.26.4 which is incompatible.\nazureml-train-automl-runtime 1.60.0 requires scikit-learn~=1.5.1, but you have scikit-learn 1.7.2 which is incompatible.\nazureml-train-automl-runtime 1.60.0 requires scipy<=1.11.0,>=1.0.0, but you have scipy 1.15.3 which is incompatible.\nazureml-train-automl-runtime 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\nazureml-responsibleai 1.60.0 requires azureml-dataset-runtime~=1.60.0, but you have azureml-dataset-runtime 1.61.0 which is incompatible.\nazureml-opendatasets 1.60.0 requires azureml-dataset-runtime[fuse,pandas]~=1.60.0, but you have azureml-dataset-runtime 1.61.0 which is incompatible.\nazureml-interpret 1.60.0 requires numpy<=1.23.5; python_version >= \"3.8\", but you have numpy 1.26.4 which is incompatible.\nazureml-datadrift 1.60.0 requires azureml-dataset-runtime[fuse,pandas]~=1.60.0, but you have azureml-dataset-runtime 1.61.0 which is incompatible.\nazureml-datadrift 1.60.0 requires matplotlib<=3.6.3,>=3.0.2, but you have matplotlib 3.7.1 which is incompatible.\nazureml-contrib-pipeline-steps 1.60.0 requires azureml-dataset-runtime~=1.60.0, but you have azureml-dataset-runtime 1.61.0 which is incompatible.\nazureml-contrib-dataset 1.60.0 requires azureml-dataset-runtime[fuse,pandas]~=1.60.0, but you have azureml-dataset-runtime 1.61.0 which is incompatible.\nazureml-automl-runtime 1.60.0 requires azureml-dataset-runtime[fuse,pandas]~=1.60.0, but you have azureml-dataset-runtime 1.61.0 which is incompatible.\nazureml-automl-runtime 1.60.0 requires numpy<=1.23.5,>=1.16.0; python_version >= \"3.8\", but you have numpy 1.26.4 which is incompatible.\nazureml-automl-runtime 1.60.0 requires scikit-learn~=1.5.1, but you have scikit-learn 1.7.2 which is incompatible.\nazureml-automl-runtime 1.60.0 requires scipy<=1.11.0,>=1.0.0, but you have scipy 1.15.3 which is incompatible.\nazureml-automl-runtime 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\nazureml-automl-dnn-nlp 1.60.0 requires torch==2.2.2, but you have torch 2.7.1 which is incompatible.\nazureml-automl-core 1.60.0 requires azureml-dataset-runtime~=1.60.0, but you have azureml-dataset-runtime 1.61.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\nCollecting pandas==2.1.1\n  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting scipy==1.10.1\n  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\nCollecting scikit-learn==1.2.2\n  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting imbalanced-learn==0.14.0\n  Using cached imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\nCollecting tzdata>=2022.1\n  Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m348.5/348.5 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting numpy>=1.22.4\n  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\nCollecting joblib>=1.1.1\n  Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nINFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\nINFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\nINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\nINFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n\u001b[31mERROR: Cannot install imbalanced-learn==0.14.0 and scikit-learn==1.2.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n\u001b[0m\nThe conflict is caused by:\n    The user requested scikit-learn==1.2.2\n    imbalanced-learn 0.14.0 depends on scikit-learn<2 and >=1.4.2\n\nTo fix this you could try to:\n1. loosen the range of package versions you've specified\n2. remove package versions to allow pip attempt to solve the dependency conflict\n\n\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n\u001b[0m"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1765910135404
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn --upgrade\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: imbalanced-learn in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (0.14.0)\nRequirement already satisfied: threadpoolctl<4,>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (3.6.0)\nRequirement already satisfied: scikit-learn<2,>=1.4.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (1.7.2)\nRequirement already satisfied: scipy<2,>=1.11.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (1.15.3)\nRequirement already satisfied: numpy<3,>=1.25.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (1.26.4)\nRequirement already satisfied: joblib<2,>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from imbalanced-learn) (1.2.0)\n"
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Data preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Balancing techniques\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, \n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    f1_score, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "# Model persistence\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# For experiment tracking (if using MLflow)\n",
        "try:\n",
        "    import mlflow\n",
        "    import mlflow.sklearn\n",
        "    MLFLOW_AVAILABLE = True\n",
        "    print(\"‚úÖ MLflow is available for experiment tracking\")\n",
        "except ImportError:\n",
        "    MLFLOW_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è MLflow not available. Install with: pip install mlflow\")\n",
        "\n",
        "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Data manipulation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-16T13:16:03.334377Z",
          "iopub.execute_input": "2025-12-16T13:16:03.334769Z",
          "iopub.status.idle": "2025-12-16T13:16:09.760633Z",
          "shell.execute_reply.started": "2025-12-16T13:16:03.334741Z",
          "shell.execute_reply": "2025-12-16T13:16:09.759570Z"
        },
        "gather": {
          "logged": 1765910181149
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä 2. Load and Explore Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azureml-dataset-runtime --upgrade"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1765906716103
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "DATA_PATH = 'fraud_dataset.csv'  \n",
        "TARGET_COLUMN = 'isFraud'\n",
        "\n",
        "print(\"üìÇ Loading dataset...\")\n",
        "print(f\"File: {DATA_PATH}\")\n",
        "print(f\"Loading started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# Connect to Azure ML Workspace\n",
        "ws = Workspace.from_config()\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "# Load dataset FROM BLOB STORAGE\n",
        "dataset = Dataset.Tabular.from_delimited_files(\n",
        "    path=[(datastore, DATA_PATH)]\n",
        ")\n",
        "\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Loading completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# Display basic information\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total rows: {df.shape[0]:,}\")\n",
        "print(f\"Total columns: {df.shape[1]:,}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(df.head())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765909163758
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine data types and missing values\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA QUALITY CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìã Column Information:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n‚ùì Missing Values:\")\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = 100 * missing / len(df)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "print(\"\\nüìä Basic Statistics:\")\n",
        "display(df.describe())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765909175997
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ 3. Analyze Class Distribution (Before Truncation)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count fraud vs non-fraud cases\n",
        "fraud_counts = df[TARGET_COLUMN].value_counts()\n",
        "fraud_pct = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"\\nüìä Original Dataset Distribution:\")\n",
        "print(f\"\\nNon-Fraud (0): {fraud_counts[0]:,} ({fraud_pct[0]:.2f}%)\")\n",
        "print(f\"Fraud (1):     {fraud_counts[1]:,} ({fraud_pct[1]:.2f}%)\")\n",
        "print(f\"\\nImbalance Ratio: 1:{fraud_counts[0]/fraud_counts[1]:.1f}\")\n",
        "print(f\"Total samples: {len(df):,}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar plot\n",
        "fraud_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Class (0=Non-Fraud, 1=Fraud)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(fraud_counts):\n",
        "    axes[0].text(i, v + max(fraud_counts)*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(fraud_counts, labels=['Non-Fraud', 'Fraud'], autopct='%1.2f%%',\n",
        "            colors=['#2ecc71', '#e74c3c'], startangle=90, textprops={'fontweight': 'bold'})\n",
        "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è This dataset is highly imbalanced - balancing is necessary!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765909180854
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÇÔ∏è 4. Strategic Dataset Truncation\n",
        "\n",
        "**Strategy:**\n",
        "- Keep ALL fraud cases (minority class)\n",
        "- Sample 500,000 non-fraud cases (majority class)\n",
        "- This reduces computational load while preserving all fraud patterns"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"DATASET TRUNCATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate fraud and non-fraud cases\n",
        "fraud_cases = df[df[TARGET_COLUMN] == 1]\n",
        "non_fraud_cases = df[df[TARGET_COLUMN] == 0]\n",
        "\n",
        "print(f\"\\nüìä Original Split:\")\n",
        "print(f\"Fraud cases:     {len(fraud_cases):,}\")\n",
        "print(f\"Non-fraud cases: {len(non_fraud_cases):,}\")\n",
        "\n",
        "# Keep all frauds, sample non-frauds\n",
        "NON_FRAUD_SAMPLE_SIZE = 500_000\n",
        "\n",
        "print(f\"\\n‚úÇÔ∏è Truncation Strategy:\")\n",
        "print(f\"‚úì Keep ALL {len(fraud_cases):,} fraud cases\")\n",
        "print(f\"‚úì Sample {NON_FRAUD_SAMPLE_SIZE:,} non-fraud cases\")\n",
        "\n",
        "# Random sampling of non-fraud cases\n",
        "non_fraud_sampled = non_fraud_cases.sample(\n",
        "    n=min(NON_FRAUD_SAMPLE_SIZE, len(non_fraud_cases)),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine truncated datasets\n",
        "df_truncated = pd.concat([fraud_cases, non_fraud_sampled], axis=0)\n",
        "df_truncated = df_truncated.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
        "\n",
        "print(f\"\\n‚úÖ Truncation Complete!\")\n",
        "print(f\"\\nüìä Truncated Dataset:\")\n",
        "print(f\"Total rows: {len(df_truncated):,}\")\n",
        "print(f\"Fraud:      {len(df_truncated[df_truncated[TARGET_COLUMN] == 1]):,}\")\n",
        "print(f\"Non-fraud:  {len(df_truncated[df_truncated[TARGET_COLUMN] == 0]):,}\")\n",
        "\n",
        "truncated_ratio = len(df_truncated[df_truncated[TARGET_COLUMN] == 0]) / len(df_truncated[df_truncated[TARGET_COLUMN] == 1])\n",
        "print(f\"\\nNew Imbalance Ratio: 1:{truncated_ratio:.1f}\")\n",
        "print(f\"Size reduction: {100 * (1 - len(df_truncated)/len(df)):.1f}%\")\n",
        "\n",
        "# Memory cleanup\n",
        "del df, fraud_cases, non_fraud_cases, non_fraud_sampled\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nüßπ Memory cleaned up\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765909189916
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß 5. Feature Engineering & Preprocessing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_truncated is already loaded\n",
        "TARGET_COLUMN = 'isFraud'  # Replace with your actual target column name\n",
        "\n",
        "# 1Ô∏è‚É£ Identify categorical columns\n",
        "categorical_cols = df_truncated.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = df_truncated.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 2Ô∏è‚É£ Handle missing values\n",
        "print(\"=\"*60)\n",
        "print(\"FEATURE ENGINEERING - Missing Values\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if df_truncated[col].isnull().sum() > 0:\n",
        "        df_truncated[col].fillna(df_truncated[col].median(), inplace=True)\n",
        "        print(f\"Filled missing values in {col} with median\")\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df_truncated[col].isnull().sum() > 0:\n",
        "        df_truncated[col].fillna(df_truncated[col].mode()[0], inplace=True)\n",
        "        print(f\"Filled missing values in {col} with mode\")\n",
        "\n",
        "# 3Ô∏è‚É£ Encode categorical columns using Target Encoding\n",
        "!pip install -q category_encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"\\nüî§ Encoding {len(categorical_cols)} categorical columns using Target Encoding...\")\n",
        "    encoder = ce.TargetEncoder(cols=categorical_cols)\n",
        "    df_truncated[categorical_cols] = encoder.fit_transform(df_truncated[categorical_cols], df_truncated[TARGET_COLUMN])\n",
        "    print(\"‚úì Encoding complete\")\n",
        "else:\n",
        "    print(\"No categorical columns to encode\")\n",
        "\n",
        "# 4Ô∏è‚É£ Optional: Scale numeric columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_truncated[numeric_cols] = scaler.fit_transform(df_truncated[numeric_cols])\n",
        "df_processed = df_truncated.copy()\n",
        "\n",
        "print(\"\\n‚úì Feature scaling complete\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765909326354
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ 6. Data Balancing with SMOTE\n",
        "\n",
        "**Multiple balancing strategies available:**\n",
        "- **SMOTE**: Synthetic Minority Over-sampling Technique\n",
        "- **ADASYN**: Adaptive Synthetic Sampling\n",
        "- **BorderlineSMOTE**: Focus on borderline cases\n",
        "- **SMOTETomek**: SMOTE + Tomek Links cleaning\n",
        "\n",
        "We'll use **SMOTE** with customizable ratio (40:60, 45:55, or 50:50)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "print(\"=\"*60)\n",
        "print(\"DATA BALANCING WITH SMOTE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_processed.drop(columns=[TARGET_COLUMN])\n",
        "y = df_processed[TARGET_COLUMN]\n",
        "\n",
        "print(f\"\\nüìä Before Balancing:\")\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution:\")\n",
        "print(y.value_counts())\n",
        "print(f\"Fraud ratio: {100 * y.mean():.2f}%\")\n",
        "\n",
        "# Choose your desired balance ratio\n",
        "# Options: 0.4 (40%), 0.45 (45%), 0.5 (50%)\n",
        "DESIRED_FRAUD_RATIO = 0.40  # ‚ö†Ô∏è ADJUST THIS (0.40 = 40/60 split)\n",
        "\n",
        "# Calculate sampling strategy\n",
        "# sampling_strategy: ratio of minority class to majority class after resampling\n",
        "# For 40% fraud: we want fraud/(non-fraud) = 0.4/0.6 = 0.667\n",
        "sampling_strategy = DESIRED_FRAUD_RATIO / (1 - DESIRED_FRAUD_RATIO)\n",
        "\n",
        "print(f\"\\nüéØ Target Balance:\")\n",
        "print(f\"Desired fraud ratio: {DESIRED_FRAUD_RATIO*100:.0f}%\")\n",
        "print(f\"Sampling strategy: {sampling_strategy:.3f}\")\n",
        "\n",
        "# Apply SMOTE\n",
        "print(f\"\\n‚öôÔ∏è Applying SMOTE...\")\n",
        "smote = SMOTE(\n",
        "    sampling_strategy=sampling_strategy,\n",
        "    random_state=42,\n",
        "    k_neighbors=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "\n",
        "print(f\"\\n‚úÖ SMOTE Complete!\")\n",
        "print(f\"\\nüìä After Balancing:\")\n",
        "print(f\"Features shape: {X_balanced.shape}\")\n",
        "print(f\"Target distribution:\")\n",
        "print(pd.Series(y_balanced).value_counts())\n",
        "fraud_pct_balanced = 100 * pd.Series(y_balanced).mean()\n",
        "print(f\"\\nFraud ratio: {fraud_pct_balanced:.2f}%\")\n",
        "print(f\"Total samples: {len(X_balanced):,}\")\n",
        "print(f\"Synthetic samples created: {len(X_balanced) - len(X):,}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "gather": {
          "logged": 1765909365808
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the balancing effect\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Before SMOTE\n",
        "y.value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Before SMOTE', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(y.value_counts()):\n",
        "    axes[0].text(i, v + max(y.value_counts())*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "# After SMOTE\n",
        "pd.Series(y_balanced).value_counts().plot(kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c'])\n",
        "axes[1].set_title('After SMOTE', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(pd.Series(y_balanced).value_counts()):\n",
        "    axes[1].text(i, v + max(pd.Series(y_balanced).value_counts())*0.02, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìà Dataset is now balanced and ready for training!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÄ 7. Train-Test Split & Scaling"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TRAIN-TEST SPLIT & FEATURE SCALING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Split the balanced dataset\n",
        "TEST_SIZE = 0.2  # 80% train, 20% test\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_balanced, y_balanced,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_balanced  # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data split complete:\")\n",
        "print(f\"\\nTraining set:   {X_train.shape[0]:,} samples ({100*(1-TEST_SIZE):.0f}%)\")\n",
        "print(f\"Test set:       {X_test.shape[0]:,} samples ({100*TEST_SIZE:.0f}%)\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "print(f\"Train - Fraud: {pd.Series(y_train).mean()*100:.2f}%\")\n",
        "print(f\"Test  - Fraud: {pd.Series(y_test).mean()*100:.2f}%\")\n",
        "\n",
        "# Feature Scaling (Important for many algorithms)\n",
        "print(f\"\\n‚öôÔ∏è Applying feature scaling...\")\n",
        "\n",
        "# RobustScaler is better for data with outliers (common in fraud detection)\n",
        "scaler = RobustScaler()\n",
        "# Alternative: StandardScaler() for normal distribution\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Scaling complete using RobustScaler\")\n",
        "print(\"\\nüì¶ Data is now ready for model training!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ 8. Model Training & Evaluation\n",
        "\n",
        "We'll train multiple models and compare their performance:\n",
        "1. Logistic Regression (baseline)\n",
        "2. Random Forest\n",
        "3. Gradient Boosting\n",
        "4. XGBoost\n",
        "5. LightGBM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MLflow (if available)\n",
        "if MLFLOW_AVAILABLE:\n",
        "    mlflow.set_experiment(\"Fraud_Detection_Training\")\n",
        "    print(\"‚úÖ MLflow experiment tracking enabled\\n\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to evaluate models\n",
        "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation function\n",
        "    \"\"\"\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{model_name} - EVALUATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "    \n",
        "    # Classification Report\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Non-Fraud', 'Fraud']))\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Confusion Matrix Heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "                xticklabels=['Non-Fraud', 'Fraud'],\n",
        "                yticklabels=['Non-Fraud', 'Fraud'])\n",
        "    axes[0].set_title(f'{model_name} - Confusion Matrix', fontweight='bold')\n",
        "    axes[0].set_ylabel('True Label')\n",
        "    axes[0].set_xlabel('Predicted Label')\n",
        "    \n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
        "    axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
        "    axes[1].set_xlabel('False Positive Rate')\n",
        "    axes[1].set_ylabel('True Positive Rate')\n",
        "    axes[1].set_title(f'{model_name} - ROC Curve', fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Evaluation function ready\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Logistic Regression (Baseline)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüîµ Training Logistic Regression...\")\n",
        "\n",
        "if MLFLOW_AVAILABLE:\n",
        "    with mlflow.start_run(run_name=\"Logistic_Regression\"):\n",
        "        lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "        lr_model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        lr_metrics = evaluate_model(lr_model, X_test_scaled, y_test, \"Logistic Regression\")\n",
        "        \n",
        "        # Log to MLflow\n",
        "        mlflow.log_params({\"model\": \"LogisticRegression\", \"max_iter\": 1000})\n",
        "        mlflow.log_metrics(lr_metrics)\n",
        "        mlflow.sklearn.log_model(lr_model, \"model\")\n",
        "else:\n",
        "    lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "    lr_model.fit(X_train_scaled, y_train)\n",
        "    lr_metrics = evaluate_model(lr_model, X_test_scaled, y_test, \"Logistic Regression\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Random Forest"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüå≤ Training Random Forest...\")\n",
        "\n",
        "if MLFLOW_AVAILABLE:\n",
        "    with mlflow.start_run(run_name=\"Random_Forest\"):\n",
        "        rf_model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=15,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        rf_metrics = evaluate_model(rf_model, X_test_scaled, y_test, \"Random Forest\")\n",
        "        \n",
        "        mlflow.log_params({\n",
        "            \"model\": \"RandomForest\",\n",
        "            \"n_estimators\": 100,\n",
        "            \"max_depth\": 15\n",
        "        })\n",
        "        mlflow.log_metrics(rf_metrics)\n",
        "        mlflow.sklearn.log_model(rf_model, \"model\")\n",
        "else:\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    rf_model.fit(X_train_scaled, y_train)\n",
        "    rf_metrics = evaluate_model(rf_model, X_test_scaled, y_test, \"Random Forest\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 XGBoost"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n‚ö° Training XGBoost...\")\n",
        "\n",
        "if MLFLOW_AVAILABLE:\n",
        "    with mlflow.start_run(run_name=\"XGBoost\"):\n",
        "        xgb_model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "        xgb_model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        xgb_metrics = evaluate_model(xgb_model, X_test_scaled, y_test, \"XGBoost\")\n",
        "        \n",
        "        mlflow.log_params({\n",
        "            \"model\": \"XGBoost\",\n",
        "            \"n_estimators\": 100,\n",
        "            \"max_depth\": 8,\n",
        "            \"learning_rate\": 0.1\n",
        "        })\n",
        "        mlflow.log_metrics(xgb_metrics)\n",
        "        mlflow.sklearn.log_model(xgb_model, \"model\")\n",
        "else:\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    xgb_model.fit(X_train_scaled, y_train)\n",
        "    xgb_metrics = evaluate_model(xgb_model, X_test_scaled, y_test, \"XGBoost\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 LightGBM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüí° Training LightGBM...\")\n",
        "\n",
        "if MLFLOW_AVAILABLE:\n",
        "    with mlflow.start_run(run_name=\"LightGBM\"):\n",
        "        lgbm_model = LGBMClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbose=-1\n",
        "        )\n",
        "        lgbm_model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        lgbm_metrics = evaluate_model(lgbm_model, X_test_scaled, y_test, \"LightGBM\")\n",
        "        \n",
        "        mlflow.log_params({\n",
        "            \"model\": \"LightGBM\",\n",
        "            \"n_estimators\": 100,\n",
        "            \"max_depth\": 8,\n",
        "            \"learning_rate\": 0.1\n",
        "        })\n",
        "        mlflow.log_metrics(lgbm_metrics)\n",
        "        mlflow.sklearn.log_model(lgbm_model, \"model\")\n",
        "else:\n",
        "    lgbm_model = LGBMClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    )\n",
        "    lgbm_model.fit(X_train_scaled, y_train)\n",
        "    lgbm_metrics = evaluate_model(lgbm_model, X_test_scaled, y_test, \"LightGBM\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä 9. Model Comparison"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create comparison dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM'],\n",
        "    'Accuracy': [lr_metrics['accuracy'], rf_metrics['accuracy'], \n",
        "                 xgb_metrics['accuracy'], lgbm_metrics['accuracy']],\n",
        "    'Precision': [lr_metrics['precision'], rf_metrics['precision'], \n",
        "                  xgb_metrics['precision'], lgbm_metrics['precision']],\n",
        "    'Recall': [lr_metrics['recall'], rf_metrics['recall'], \n",
        "               xgb_metrics['recall'], lgbm_metrics['recall']],\n",
        "    'F1-Score': [lr_metrics['f1_score'], rf_metrics['f1_score'], \n",
        "                 xgb_metrics['f1_score'], lgbm_metrics['f1_score']],\n",
        "    'ROC-AUC': [lr_metrics['roc_auc'], rf_metrics['roc_auc'], \n",
        "                xgb_metrics['roc_auc'], lgbm_metrics['roc_auc']]\n",
        "})\n",
        "\n",
        "# Sort by F1-Score (good balance for fraud detection)\n",
        "results_df = results_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nüìä Performance Metrics Comparison:\")\n",
        "display(results_df.style.background_gradient(cmap='RdYlGn', subset=['F1-Score', 'ROC-AUC']))\n",
        "\n",
        "# Identify best model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
        "print(f\"   F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
        "print(f\"   ROC-AUC:  {results_df.iloc[0]['ROC-AUC']:.4f}\")\n",
        "\n",
        "# Visual comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "for idx, (metric, color) in enumerate(zip(metrics_to_plot, colors)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    results_df.plot(x='Model', y=metric, kind='bar', ax=ax, color=color, legend=False)\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
        "    ax.set_ylim([0, 1.05])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(results_df[metric]):\n",
        "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ 10. Save Best Model & Artifacts"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING MODEL & ARTIFACTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create models directory\n",
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Determine best model\n",
        "model_map = {\n",
        "    'Logistic Regression': lr_model,\n",
        "    'Random Forest': rf_model,\n",
        "    'XGBoost': xgb_model,\n",
        "    'LightGBM': lgbm_model\n",
        "}\n",
        "\n",
        "best_model = model_map[best_model_name]\n",
        "\n",
        "# Save the best model\n",
        "model_filename = f'models/fraud_detection_best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
        "joblib.dump(best_model, model_filename)\n",
        "print(f\"\\n‚úÖ Best model saved: {model_filename}\")\n",
        "\n",
        "# Save the scaler\n",
        "scaler_filename = 'models/scaler.pkl'\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "print(f\"‚úÖ Scaler saved: {scaler_filename}\")\n",
        "\n",
        "# Save feature names\n",
        "feature_names = X.columns.tolist()\n",
        "with open('models/feature_names.txt', 'w') as f:\n",
        "    f.write('\\n'.join(feature_names))\n",
        "print(f\"‚úÖ Feature names saved: models/feature_names.txt\")\n",
        "\n",
        "# Save training metadata\n",
        "metadata = {\n",
        "    'model_name': best_model_name,\n",
        "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'n_features': len(feature_names),\n",
        "    'n_train_samples': len(X_train),\n",
        "    'n_test_samples': len(X_test),\n",
        "    'fraud_ratio_after_smote': f\"{DESIRED_FRAUD_RATIO*100:.0f}%\",\n",
        "    'test_accuracy': results_df[results_df['Model'] == best_model_name]['Accuracy'].values[0],\n",
        "    'test_f1_score': results_df[results_df['Model'] == best_model_name]['F1-Score'].values[0],\n",
        "    'test_roc_auc': results_df[results_df['Model'] == best_model_name]['ROC-AUC'].values[0]\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('models/model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "print(f\"‚úÖ Metadata saved: models/model_metadata.json\")\n",
        "\n",
        "# Save all models for comparison\n",
        "print(\"\\nüíæ Saving all models for future reference...\")\n",
        "for model_name, model_obj in model_map.items():\n",
        "    filename = f'models/{model_name.lower().replace(\" \", \"_\")}.pkl'\n",
        "    joblib.dump(model_obj, filename)\n",
        "    print(f\"  ‚úì {filename}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ ALL ARTIFACTS SAVED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nSaved files:\")\n",
        "print(\"üìÅ models/\")\n",
        "print(\"  ‚îú‚îÄ‚îÄ fraud_detection_best_model_*.pkl\")\n",
        "print(\"  ‚îú‚îÄ‚îÄ scaler.pkl\")\n",
        "print(\"  ‚îú‚îÄ‚îÄ feature_names.txt\")\n",
        "print(\"  ‚îú‚îÄ‚îÄ model_metadata.json\")\n",
        "print(\"  ‚îî‚îÄ‚îÄ [all trained models].pkl\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ 11. Quick Inference Test"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING MODEL INFERENCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(model_filename)\n",
        "loaded_scaler = joblib.load(scaler_filename)\n",
        "\n",
        "print(\"\\n‚úÖ Model and scaler loaded successfully\")\n",
        "\n",
        "# Test on a few samples\n",
        "n_test_samples = 5\n",
        "test_samples = X_test.iloc[:n_test_samples]\n",
        "test_labels = y_test.iloc[:n_test_samples]\n",
        "\n",
        "# Scale and predict\n",
        "test_samples_scaled = loaded_scaler.transform(test_samples)\n",
        "predictions = loaded_model.predict(test_samples_scaled)\n",
        "probabilities = loaded_model.predict_proba(test_samples_scaled)\n",
        "\n",
        "print(f\"\\nüß™ Testing on {n_test_samples} random samples:\\n\")\n",
        "for i in range(n_test_samples):\n",
        "    actual = \"FRAUD\" if test_labels.iloc[i] == 1 else \"LEGITIMATE\"\n",
        "    predicted = \"FRAUD\" if predictions[i] == 1 else \"LEGITIMATE\"\n",
        "    fraud_prob = probabilities[i][1] * 100\n",
        "    \n",
        "    match = \"‚úÖ\" if actual == predicted else \"‚ùå\"\n",
        "    print(f\"Sample {i+1}: {match}\")\n",
        "    print(f\"  Actual: {actual}\")\n",
        "    print(f\"  Predicted: {predicted} (Fraud probability: {fraud_prob:.2f}%)\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n‚úÖ Model is ready for deployment!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìã 12. Summary & Next Steps"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" \"*20 + \"PROJECT SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n‚úÖ COMPLETED STEPS:\")\n",
        "print(\"\\n1. ‚úì Data Loading & Exploration\")\n",
        "print(f\"   - Original dataset: 6M rows\")\n",
        "print(f\"   - Highly imbalanced classes\")\n",
        "\n",
        "print(\"\\n2. ‚úì Strategic Data Truncation\")\n",
        "print(f\"   - Kept all fraud cases\")\n",
        "print(f\"   - Sampled 500k non-fraud cases\")\n",
        "print(f\"   - Reduced dataset size by ~{100 * (1 - len(df_truncated)/6_000_000):.0f}%\")\n",
        "\n",
        "print(\"\\n3. ‚úì Data Balancing with SMOTE\")\n",
        "print(f\"   - Target ratio: {DESIRED_FRAUD_RATIO*100:.0f}/{(1-DESIRED_FRAUD_RATIO)*100:.0f}\")\n",
        "print(f\"   - Synthetic samples created: {len(X_balanced) - len(df_truncated):,}\")\n",
        "\n",
        "print(\"\\n4. ‚úì Feature Engineering & Preprocessing\")\n",
        "print(f\"   - Features: {len(feature_names)}\")\n",
        "print(f\"   - Scaling: RobustScaler\")\n",
        "print(f\"   - Missing values handled\")\n",
        "\n",
        "print(\"\\n5. ‚úì Model Training & Evaluation\")\n",
        "print(f\"   - Models trained: 4\")\n",
        "print(f\"   - Best model: {best_model_name}\")\n",
        "print(f\"   - F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
        "print(f\"   - ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f}\")\n",
        "\n",
        "print(\"\\n6. ‚úì Model & Artifacts Saved\")\n",
        "print(f\"   - Best model: {model_filename}\")\n",
        "print(f\"   - Scaler & metadata saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìå NEXT STEPS FOR YOUR CDDA PROJECT:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "next_steps = [\n",
        "    \"1. Data Ingestion (Azure)\",\n",
        "    \"   ‚îî‚îÄ Upload dataset to Azure Blob Storage\",\n",
        "    \"   ‚îî‚îÄ Set up Azure Data Factory for data pipeline\",\n",
        "    \"\",\n",
        "    \"2. Model Deployment (Azure ML)\",\n",
        "    \"   ‚îî‚îÄ Register model in Azure ML\",\n",
        "    \"   ‚îî‚îÄ Deploy as REST API endpoint\",\n",
        "    \"   ‚îî‚îÄ Test endpoint with sample transactions\",\n",
        "    \"\",\n",
        "    \"3. CI/CD Pipeline (Azure DevOps)\",\n",
        "    \"   ‚îî‚îÄ Create GitHub/Azure DevOps pipeline\",\n",
        "    \"   ‚îî‚îÄ Automate model retraining\",\n",
        "    \"   ‚îî‚îÄ Set up automated deployment\",\n",
        "    \"\",\n",
        "    \"4. Monitoring & Alerting (Azure Monitor)\",\n",
        "    \"   ‚îî‚îÄ Set up Application Insights\",\n",
        "    \"   ‚îî‚îÄ Create alerts for model drift\",\n",
        "    \"   ‚îî‚îÄ Monitor API performance\",\n",
        "    \"\",\n",
        "    \"5. Visualization (Power BI)\",\n",
        "    \"   ‚îî‚îÄ Connect to Azure SQL/Storage\",\n",
        "    \"   ‚îî‚îÄ Create fraud detection dashboard\",\n",
        "    \"   ‚îî‚îÄ Add real-time transaction monitoring\",\n",
        "    \"\",\n",
        "    \"6. User Interface (Streamlit)\",\n",
        "    \"   ‚îî‚îÄ Build web interface for predictions\",\n",
        "    \"   ‚îî‚îÄ Add transaction upload functionality\",\n",
        "    \"   ‚îî‚îÄ Display results and visualizations\",\n",
        "    \"\",\n",
        "    \"7. Security & Governance\",\n",
        "    \"   ‚îî‚îÄ Implement Azure Key Vault for secrets\",\n",
        "    \"   ‚îî‚îÄ Set up RBAC (Role-Based Access Control)\",\n",
        "    \"   ‚îî‚îÄ Document data governance policies\"\n",
        "]\n",
        "\n",
        "for step in next_steps:\n",
        "    print(step)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéì CDDA PROJECT CHECKLIST (from Lab requirements):\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checklist = [\n",
        "    (\"‚úÖ\", \"Data Ingestion (ready for Azure)\"),\n",
        "    (\"‚úÖ\", \"Data Storage (model artifacts ready)\"),\n",
        "    (\"‚úÖ\", \"Data Processing (SMOTE balancing done)\"),\n",
        "    (\"‚úÖ\", \"Data Balancing (40/60 ratio achieved)\"),\n",
        "    (\"‚úÖ\", \"Model Training (multiple algorithms)\"),\n",
        "    (\"‚úÖ\", \"Experiment Tracking (MLflow ready)\"),\n",
        "    (\"‚è≥\", \"Deployment (next: Azure ML endpoint)\"),\n",
        "    (\"‚è≥\", \"Inference Interface (next: Streamlit)\"),\n",
        "    (\"‚è≥\", \"CI/CD Pipeline (next: Azure DevOps)\"),\n",
        "    (\"‚è≥\", \"Monitoring (next: Azure Monitor)\"),\n",
        "    (\"‚è≥\", \"Security & Governance (next: Key Vault)\"),\n",
        "    (\"‚è≥\", \"Dashboard (next: Power BI)\"),\n",
        "]\n",
        "\n",
        "for status, item in checklist:\n",
        "    print(f\"{status} {item}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° TIPS FOR SUCCESS:\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "1. Document everything in your project report\n",
        "2. Take screenshots of Azure services configuration\n",
        "3. Save all connection strings in Azure Key Vault\n",
        "4. Create a comprehensive Power BI dashboard\n",
        "5. Prepare a demo video showing the complete pipeline\n",
        "6. Test your deployment thoroughly before presentation\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üéâ Model training complete! Good luck with your CDDA project!\")\n",
        "print(\"=\"*70)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "trusted": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
#!/usr/bin/env python
# coding: utf-8

# # Fraud Detection - Data Preprocessing & Model Training
# ## Azure Cloud Data-Driven Application Project
# 
# **Project:** Detection of fraud in financial transactions  
# **Dataset:** 6M rows ‚Üí Truncated & Balanced for optimal training  
# **Goal:** Train a high-performance fraud detection model
# 
# ---
# 
# ## Pipeline Overview:
# 1. **Data Loading & Exploration**
# 2. **Strategic Truncation** (All frauds + 500k non-frauds)
# 3. **Data Balancing with SMOTE** (40/60 or customizable ratio)
# 4. **Feature Engineering & Preprocessing**
# 5. **Model Training** (Multiple algorithms)
# 6. **Model Evaluation & Comparison**
# 7. **Export Best Model** for deployment

# ## üì¶ 1. Import Required Libraries

# In[1]:


pip install mlflow


# In[2]:


get_ipython().run_line_magic('pip', 'install xgboost lightgbm')
get_ipython().run_line_magic('pip', 'install imbalanced-learn')


# In[29]:


# Data manipulation
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

# Data preprocessing
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.utils import resample

# Balancing techniques
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek, SMOTEENN

# Machine Learning models
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Model evaluation
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, roc_curve,
    precision_recall_curve, average_precision_score
)

# Model persistence
import joblib
import pickle

# For experiment tracking (if using MLflow)
try:
    import mlflow
    import mlflow.sklearn
    MLFLOW_AVAILABLE = True
    print("‚úÖ MLflow is available for experiment tracking")
except ImportError:
    MLFLOW_AVAILABLE = False
    print("‚ö†Ô∏è MLflow not available. Install with: pip install mlflow")

print("\n‚úÖ All libraries imported successfully!")
print(f"Pandas version: {pd.__version__}")
print(f"NumPy version: {np.__version__}")


# ## üìä 2. Load and Explore Dataset

# In[5]:


get_ipython().system('pip install azureml-dataset-runtime --upgrade')


# In[30]:


# Configuration
from datetime import datetime
import pandas as pd
from azureml.core import Workspace, Dataset

DATA_PATH = 'fraud_dataset.csv'  
TARGET_COLUMN = 'isFraud'

print("üìÇ Loading dataset...")
print(f"File: {DATA_PATH}")
print(f"Loading started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Connect to Azure ML Workspace
ws = Workspace.from_config()
datastore = ws.get_default_datastore()

# Load dataset FROM BLOB STORAGE
dataset = Dataset.Tabular.from_delimited_files(
    path=[(datastore, DATA_PATH)]
)

df = dataset.to_pandas_dataframe()

print(f"‚úÖ Dataset loaded successfully!")
print(f"Loading completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Display basic information
print("="*60)
print("DATASET OVERVIEW")
print("="*60)
print(f"Total rows: {df.shape[0]:,}")
print(f"Total columns: {df.shape[1]:,}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print("\nFirst few rows:")
display(df.head())


# In[31]:


# Examine data types and missing values
print("\n" + "="*60)
print("DATA QUALITY CHECK")
print("="*60)

print("\nüìã Column Information:")
df.info()

print("\n‚ùì Missing Values:")
missing = df.isnull().sum()
missing_pct = 100 * missing / len(df)
missing_df = pd.DataFrame({
    'Missing Count': missing,
    'Percentage': missing_pct
}).sort_values('Missing Count', ascending=False)
print(missing_df[missing_df['Missing Count'] > 0])

print("\nüìä Basic Statistics:")
display(df.describe())


# ## üéØ 3. Analyze Class Distribution (Before Truncation)

# In[32]:


import matplotlib.pyplot as plt
print("="*60)
print("CLASS DISTRIBUTION ANALYSIS")
print("="*60)

# Count fraud vs non-fraud cases
fraud_counts = df[TARGET_COLUMN].value_counts()
fraud_pct = df[TARGET_COLUMN].value_counts(normalize=True) * 100

print("\nüìä Original Dataset Distribution:")
print(f"\nNon-Fraud (0): {fraud_counts[0]:,} ({fraud_pct[0]:.2f}%)")
print(f"Fraud (1):     {fraud_counts[1]:,} ({fraud_pct[1]:.2f}%)")
print(f"\nImbalance Ratio: 1:{fraud_counts[0]/fraud_counts[1]:.1f}")
print(f"Total samples: {len(df):,}")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Bar plot
fraud_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])
axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Class (0=Non-Fraud, 1=Fraud)')
axes[0].set_ylabel('Count')
axes[0].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)
for i, v in enumerate(fraud_counts):
    axes[0].text(i, v + max(fraud_counts)*0.02, f'{v:,}', ha='center', fontweight='bold')

# Pie chart
axes[1].pie(fraud_counts, labels=['Non-Fraud', 'Fraud'], autopct='%1.2f%%',
            colors=['#2ecc71', '#e74c3c'], startangle=90, textprops={'fontweight': 'bold'})
axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print("\n‚ö†Ô∏è This dataset is highly imbalanced - balancing is necessary!")


# ## ‚úÇÔ∏è 4. Strategic Dataset Truncation
# 
# **Strategy:**
# - Keep ALL fraud cases (minority class)
# - Sample 500,000 non-fraud cases (majority class)
# - This reduces computational load while preserving all fraud patterns

# In[33]:


print("="*60)
print("DATASET TRUNCATION")
print("="*60)

# Separate fraud and non-fraud cases
fraud_cases = df[df[TARGET_COLUMN] == 1]
non_fraud_cases = df[df[TARGET_COLUMN] == 0]

print(f"\nüìä Original Split:")
print(f"Fraud cases:     {len(fraud_cases):,}")
print(f"Non-fraud cases: {len(non_fraud_cases):,}")

# Keep all frauds, sample non-frauds
NON_FRAUD_SAMPLE_SIZE = 500_000

print(f"\n‚úÇÔ∏è Truncation Strategy:")
print(f"‚úì Keep ALL {len(fraud_cases):,} fraud cases")
print(f"‚úì Sample {NON_FRAUD_SAMPLE_SIZE:,} non-fraud cases")

# Random sampling of non-fraud cases
non_fraud_sampled = non_fraud_cases.sample(
    n=min(NON_FRAUD_SAMPLE_SIZE, len(non_fraud_cases)),
    random_state=42
)

# Combine truncated datasets
df_truncated = pd.concat([fraud_cases, non_fraud_sampled], axis=0)
df_truncated = df_truncated.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle

print(f"\n‚úÖ Truncation Complete!")
print(f"\nüìä Truncated Dataset:")
print(f"Total rows: {len(df_truncated):,}")
print(f"Fraud:      {len(df_truncated[df_truncated[TARGET_COLUMN] == 1]):,}")
print(f"Non-fraud:  {len(df_truncated[df_truncated[TARGET_COLUMN] == 0]):,}")

truncated_ratio = len(df_truncated[df_truncated[TARGET_COLUMN] == 0]) / len(df_truncated[df_truncated[TARGET_COLUMN] == 1])
print(f"\nNew Imbalance Ratio: 1:{truncated_ratio:.1f}")
print(f"Size reduction: {100 * (1 - len(df_truncated)/len(df)):.1f}%")

# Memory cleanup
del df, fraud_cases, non_fraud_cases, non_fraud_sampled
import gc
gc.collect()

print("\nüßπ Memory cleaned up")


# ## üîß 5. Feature Engineering & Preprocessing

# In[9]:


import sys
get_ipython().system('{sys.executable} -m pip install category_encoders')


# In[34]:


import pandas as pd
import numpy as np

# Assuming df_truncated is already loaded
TARGET_COLUMN = 'isFraud'  # Replace with your actual target column name

# 1Ô∏è‚É£ Identify categorical columns
categorical_cols = df_truncated.select_dtypes(include=['object', 'category']).columns
numeric_cols = df_truncated.select_dtypes(include=[np.number]).columns

# 2Ô∏è‚É£ Handle missing values
print("="*60)
print("FEATURE ENGINEERING - Missing Values")
print("="*60)

for col in numeric_cols:
    if df_truncated[col].isnull().sum() > 0:
        df_truncated[col].fillna(df_truncated[col].median(), inplace=True)
        print(f"Filled missing values in {col} with median")

for col in categorical_cols:
    if df_truncated[col].isnull().sum() > 0:
        df_truncated[col].fillna(df_truncated[col].mode()[0], inplace=True)
        print(f"Filled missing values in {col} with mode")

# 3Ô∏è‚É£ Encode categorical columns using Target Encoding
get_ipython().system('pip install -q category_encoders')
import category_encoders as ce

if len(categorical_cols) > 0:
    print(f"\nüî§ Encoding {len(categorical_cols)} categorical columns using Target Encoding...")
    encoder = ce.TargetEncoder(cols=categorical_cols)
    df_truncated[categorical_cols] = encoder.fit_transform(df_truncated[categorical_cols], df_truncated[TARGET_COLUMN])
    print("‚úì Encoding complete")
else:
    print("No categorical columns to encode")

# 4Ô∏è‚É£ Optional: Scale numeric columns
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_truncated[numeric_cols] = scaler.fit_transform(df_truncated[numeric_cols])
df_processed = df_truncated.copy()

print("\n‚úì Feature scaling complete")


# ## üéØ 6. Data Balancing with SMOTE
# 
# **Multiple balancing strategies available:**
# - **SMOTE**: Synthetic Minority Over-sampling Technique
# - **ADASYN**: Adaptive Synthetic Sampling
# - **BorderlineSMOTE**: Focus on borderline cases
# - **SMOTETomek**: SMOTE + Tomek Links cleaning
# 
# We'll use **SMOTE** with customizable ratio (40:60, 45:55, or 50:50)

# In[11]:


import imblearn
from imblearn.over_sampling import SMOTE
print(imblearn.__file__)
print(SMOTE)


# In[132]:


from sklearn.model_selection import train_test_split

# Split original dataset BEFORE balancing
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)


# In[133]:


from imblearn.over_sampling import SMOTE
print("="*60)
print("DATA BALANCING WITH SMOTE")
print("="*60)

# Separate features and target
X = df_processed.drop(columns=[TARGET_COLUMN])
y = df_processed[TARGET_COLUMN]

print(f"\nüìä Before Balancing:")
print(f"Features shape: {X.shape}")
print(f"Target distribution:")
print(y.value_counts())
print(f"Fraud ratio: {100 * y.mean():.2f}%")

# Choose your desired balance ratio
# Options: 0.4 (40%), 0.45 (45%), 0.5 (50%)
DESIRED_FRAUD_RATIO = 0.40  # ‚ö†Ô∏è ADJUST THIS (0.40 = 40/60 split)

# Calculate sampling strategy
# sampling_strategy: ratio of minority class to majority class after resampling
# For 40% fraud: we want fraud/(non-fraud) = 0.4/0.6 = 0.667
sampling_strategy = DESIRED_FRAUD_RATIO / (1 - DESIRED_FRAUD_RATIO)

print(f"\nüéØ Target Balance:")
print(f"Desired fraud ratio: {DESIRED_FRAUD_RATIO*100:.0f}%")
print(f"Sampling strategy: {sampling_strategy:.3f}")

# Apply SMOTE
print(f"\n‚öôÔ∏è Applying SMOTE...")
smote = SMOTE(
    sampling_strategy=sampling_strategy,
    random_state=42,
    k_neighbors=5)

X_balanced, y_balanced = smote.fit_resample(X, y)

print(f"\n‚úÖ SMOTE Complete!")
print(f"\nüìä After Balancing:")
print(f"Features shape: {X_balanced.shape}")
print(f"Target distribution:")
print(pd.Series(y_balanced).value_counts())
fraud_pct_balanced = 100 * pd.Series(y_balanced).mean()
print(f"\nFraud ratio: {fraud_pct_balanced:.2f}%")
print(f"Total samples: {len(X_balanced):,}")
print(f"Synthetic samples created: {len(X_balanced) - len(X):,}")


# In[134]:


# Visualize the balancing effect
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Before SMOTE
y.value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])
axes[0].set_title('Before SMOTE', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Class')
axes[0].set_ylabel('Count')
axes[0].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)
for i, v in enumerate(y.value_counts()):
    axes[0].text(i, v + max(y.value_counts())*0.02, f'{v:,}', ha='center', fontweight='bold')

# After SMOTE
pd.Series(y_balanced).value_counts().plot(kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c'])
axes[1].set_title('After SMOTE', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Class')
axes[1].set_ylabel('Count')
axes[1].set_xticklabels(['Non-Fraud', 'Fraud'], rotation=0)
for i, v in enumerate(pd.Series(y_balanced).value_counts()):
    axes[1].text(i, v + max(pd.Series(y_balanced).value_counts())*0.02, f'{v:,}', ha='center', fontweight='bold')

plt.tight_layout()
plt.show()

print("\nüìà Dataset is now balanced and ready for training!")


# In[135]:


X_balanced = X_balanced.drop(columns=leakage_cols)


# ## üîÄ 7. Train-Test Split & Scaling

# In[136]:


leakage_cols = ['nameOrig', 'nameDest', 'isFlaggedFraud']
X = X.drop(columns=leakage_cols)


# In[137]:


from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=5)
X_train_bal, y_train_bal = smote.fit_resample(X_train_orig, y_train_orig)


# In[138]:


print("="*60)
print("TRAIN-TEST SPLIT & FEATURE SCALING")
print("="*60)

# Split the balanced dataset
TEST_SIZE = 0.2  # 80% train, 20% test
RANDOM_STATE = 42

# 60% train, 20% validation, 20% test
X_temp, X_test, y_temp, y_test = train_test_split(
    X_balanced, y_balanced,
    test_size=0.2,
    random_state=42,
    stratify=y_balanced
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp,
    test_size=0.25,  # 0.25 * 0.8 = 0.2
    random_state=42,
    stratify=y_temp
)

scaler = RobustScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled   = scaler.transform(X_val)
X_test_scaled  = scaler.transform(X_test)
print(f"\n‚úÖ Data split complete:")
print(f"\nTraining set:   {X_train.shape[0]:,} samples ({100*(1-TEST_SIZE):.0f}%)")
print(f"Test set:       {X_test.shape[0]:,} samples ({100*TEST_SIZE:.0f}%)")
print(f"Number of features: {X_train.shape[1]}")

print(f"\nüìä Class distribution:")
print(f"Train - Fraud: {pd.Series(y_train).mean()*100:.2f}%")
print(f"Test  - Fraud: {pd.Series(y_test).mean()*100:.2f}%")

# Feature Scaling (Important for many algorithms)
print(f"\n‚öôÔ∏è Applying feature scaling...")

# RobustScaler is better for data with outliers (common in fraud detection)

print("Features utilis√©es :", X_train.columns.tolist())

# R√©cup√©ration des noms de features pour l'analyse apr√®s scaling
if hasattr(X_train, 'columns'):
    feature_names = X_train.columns
else:
    feature_names = [f'feature_{i}' for i in range(X_train_scaled.shape[1])]

# Cr√©er DataFrame pour statistiques
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)

# Alternative: StandardScaler() for normal distribution


print("‚úÖ Scaling complete using RobustScaler")
print("\nüì¶ Data is now ready for model training!")


# ## ü§ñ 8. Model Training & Evaluation
# 
# We'll train multiple models and compare their performance:
# 1. Logistic Regression (baseline)
# 2. Random Forest
# 3. Gradient Boosting
# 4. XGBoost
# 5. LightGBM

# In[139]:


# =============================================================================
# PARTIE 1: CONFIGURATION AZURE ML + MLFLOW
# =============================================================================

print("="*80)
print(" "*20 + "üöÄ AZURE ML - MODEL TRAINING PIPELINE")
print("="*80)

import warnings
warnings.filterwarnings('ignore')

# Imports essentiels
import pandas as pd
import numpy as np
from datetime import datetime
import json
import os

# Azure ML
from azureml.core import Workspace, Experiment, Run, Dataset, Model
from azureml.core.compute import ComputeTarget
import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient

# Machine Learning
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# M√©triques
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve
)

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

print("\n‚úÖ Toutes les biblioth√®ques import√©es avec succ√®s!")


# In[140]:


# =============================================================================
# CONNEXION AU WORKSPACE AZURE ML
# =============================================================================

print("\n" + "="*80)
print("üì° CONNEXION √Ä AZURE ML WORKSPACE")
print("="*80)

try:
    # M√©thode 1: Depuis le contexte du run (si dans Azure ML Compute)
    run_context = Run.get_context()
    
    if hasattr(run_context, 'experiment'):
        ws = run_context.experiment.workspace
        print("‚úÖ Connect√© depuis Azure ML Compute Instance")
    else:
        # M√©thode 2: Depuis config.json local
        ws = Workspace.from_config()
        print("‚úÖ Connect√© depuis configuration locale")
    
    print(f"\nüìã Workspace Details:")
    print(f"   Nom: {ws.name}")
    print(f"   R√©gion: {ws.location}")
    print(f"   Resource Group: {ws.resource_group}")
    print(f"   Subscription ID: {ws.subscription_id[:8]}...")
    
    # Configuration MLflow pour Azure ML
    mlflow_uri = ws.get_mlflow_tracking_uri()
    mlflow.set_tracking_uri(mlflow_uri)
    print(f"\nüîó MLflow Tracking URI configur√©: {mlflow_uri}")
    
    AZURE_ML_CONNECTED = True

except Exception as e:
    print(f"‚ö†Ô∏è Erreur de connexion: {str(e)}")
    print("üí° Utilisation du mode local pour d√©monstration")
    AZURE_ML_CONNECTED = False
    mlflow.set_tracking_uri("./mlruns")

# Cr√©er/R√©cup√©rer l'exp√©rience
EXPERIMENT_NAME = "fraud-detection-training"
experiment = Experiment(workspace=ws, name=EXPERIMENT_NAME) if AZURE_ML_CONNECTED else None

if AZURE_ML_CONNECTED:
    print(f"\nüß™ Exp√©rience: {EXPERIMENT_NAME}")
    mlflow.set_experiment(EXPERIMENT_NAME)
else:
    mlflow.set_experiment("fraud-detection-local")

print("\n" + "="*80)


# In[141]:


# =============================================================================
# PARTIE 2: CHARGEMENT DES DONN√âES (depuis Azure Blob Storage)
# =============================================================================

print("\n" + "="*80)
print("üìÇ CHARGEMENT DES DONN√âES DEPUIS AZURE BLOB STORAGE")
print("="*80)

# Supposons que X_train_scaled, X_test_scaled, y_train, y_test sont d√©j√† pr√©par√©s
# Si ce n'est pas le cas, d√©commentez le code ci-dessous:

"""
# Charger depuis Azure Blob Storage
datastore = ws.get_default_datastore()
dataset = Dataset.Tabular.from_delimited_files(
    path=[(datastore, 'fraud_dataset_processed.csv')]
)
df = dataset.to_pandas_dataframe()

# S√©parer features et target
X = df.drop(columns=['isFraud'])
y = df['isFraud']

# Split train/test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scaling
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
"""

print(f"‚úÖ Donn√©es charg√©es:")
print(f"   Training samples: {len(X_train_scaled):,}")
print(f"   Test samples: {len(X_test_scaled):,}")
print(f"   Features: {X_train_scaled.shape[1]}")
print(f"   Fraud ratio (train): {pd.Series(y_train).mean()*100:.2f}%")



# In[142]:


# =============================================================================
# PARTIE 3: FONCTION D'√âVALUATION COMPL√àTE
# =============================================================================

def evaluate_model_comprehensive(model, X_test, y_test, model_name="Model"):
    """
    √âvaluation compl√®te du mod√®le avec visualisations professionnelles
    
    Returns:
        dict: M√©triques de performance
        tuple: Figures matplotlib pour logging
    """
    
    # Pr√©dictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Calcul des m√©triques
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1_score': f1_score(y_test, y_pred, zero_division=0),
        'roc_auc': roc_auc_score(y_test, y_pred_proba)
    }
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    
    # M√©triques suppl√©mentaires pour fraude
    tn, fp, fn, tp = cm.ravel()
    metrics['true_negatives'] = int(tn)
    metrics['false_positives'] = int(fp)
    metrics['false_negatives'] = int(fn)
    metrics['true_positives'] = int(tp)
    metrics['fraud_detection_rate'] = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics['false_alarm_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    # Affichage des r√©sultats
    print(f"\n{'='*70}")
    print(f"üìä {model_name} - R√âSULTATS D'√âVALUATION")
    print(f"{'='*70}")
    print(f"Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"Precision: {metrics['precision']:.4f} (Qualit√© des alertes fraude)")
    print(f"Recall:    {metrics['recall']:.4f} (D√©tection des fraudes)")
    print(f"F1-Score:  {metrics['f1_score']:.4f} (Balance Precision/Recall)")
    print(f"ROC-AUC:   {metrics['roc_auc']:.4f} (Capacit√© de discrimination)")
    
    print(f"\nüéØ M√©triques m√©tier sp√©cifiques:")
    print(f"Taux de d√©tection fraude: {metrics['fraud_detection_rate']*100:.2f}%")
    print(f"Taux de fausses alertes:  {metrics['false_alarm_rate']*100:.2f}%")
    
    print(f"\nüî¢ Confusion Matrix:")
    print(f"   TN: {tn:,}  |  FP: {fp:,}")
    print(f"   FN: {fn:,}  |  TP: {tp:,}")
    
    # Visualisations
    fig = plt.figure(figsize=(18, 5))
    
    # 1. Confusion Matrix
    ax1 = plt.subplot(1, 3, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Non-Fraude', 'Fraude'],
                yticklabels=['Non-Fraude', 'Fraude'],
                ax=ax1, cbar_kws={'label': 'Nombre de cas'})
    ax1.set_title(f'{model_name}\nMatrice de Confusion', fontweight='bold', fontsize=12)
    ax1.set_ylabel('Vraie Classe')
    ax1.set_xlabel('Classe Pr√©dite')
    
    # 2. ROC Curve
    ax2 = plt.subplot(1, 3, 2)
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    ax2.plot(fpr, tpr, linewidth=2.5, 
             label=f'ROC (AUC = {metrics["roc_auc"]:.3f})', color='#e74c3c')
    ax2.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Hasard')
    ax2.set_xlabel('Taux de Faux Positifs (FPR)')
    ax2.set_ylabel('Taux de Vrais Positifs (TPR)')
    ax2.set_title(f'{model_name}\nCourbe ROC', fontweight='bold', fontsize=12)
    ax2.legend(loc='lower right')
    ax2.grid(alpha=0.3)
    
    # 3. Precision-Recall Curve
    ax3 = plt.subplot(1, 3, 3)
    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)
    ax3.plot(recall_vals, precision_vals, linewidth=2.5, color='#2ecc71')
    ax3.set_xlabel('Recall (Taux de d√©tection)')
    ax3.set_ylabel('Precision (Qualit√© des alertes)')
    ax3.set_title(f'{model_name}\nCourbe Precision-Recall', fontweight='bold', fontsize=12)
    ax3.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.show() 
    return metrics, fig

print("\n‚úÖ Fonction d'√©valuation configur√©e")


# In[104]:


leakage_cols = ['nameOrig', 'nameDest', 'isFlaggedFraud']
X = X.drop(columns=leakage_cols)


# In[143]:


# =============================================================================
# PARTIE 4: CONFIGURATION DES MOD√àLES (VERSION CORRIG√âE)
# =============================================================================

print("\n" + "="*80)
print("ü§ñ CONFIGURATION DES MOD√àLES DE MACHINE LEARNING")
print("="*80)

# IMPORTANT: V√©rifier d'abord le d√©s√©quilibre r√©el de vos donn√©es
print(f"\nüìä Analyse du d√©s√©quilibre des classes:")
fraud_count = y_train.sum()
total_count = len(y_train)
fraud_ratio = fraud_count / total_count
print(f"   ‚Ä¢ Transactions frauduleuses: {fraud_count:,} ({fraud_ratio*100:.2f}%)")
print(f"   ‚Ä¢ Transactions l√©gitimes: {total_count - fraud_count:,} ({(1-fraud_ratio)*100:.2f}%)")
print(f"   ‚Ä¢ Ratio d√©s√©quilibre: 1:{int(1/fraud_ratio)}")

# Calculer le scale_pos_weight optimal pour XGBoost
# scale_pos_weight = (nombre de n√©gatifs) / (nombre de positifs)
scale_pos_weight = (total_count - fraud_count) / fraud_count
print(f"   ‚Ä¢ scale_pos_weight optimal: {scale_pos_weight:.2f}")

# Configuration des mod√®les avec hyperparam√®tres CORRIG√âS
models_config = {
    'Logistic_Regression': {
        'model': LogisticRegression(
            max_iter=1000,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced',  # OK pour la r√©gression logistique
            solver='saga',
            penalty='l2',
            C=1.0  # R√©gularisation standard
        ),
        'description': 'Mod√®le lin√©aire baseline - Rapide et interpr√©table',
        'color': '#3498db'
    },
    
    'Random_Forest': {
        'model': RandomForestClassifier(
            n_estimators=100,  # R√©duit de 150 √† 100
            max_depth=15,      # R√©duit de 20 √† 15 (√©vite l'overfitting)
            min_samples_split=10,  # Augment√© de 5 √† 10
            min_samples_leaf=4,    # Augment√© de 2 √† 4
            max_features='sqrt',
            random_state=42,
            n_jobs=-1,
            class_weight='balanced_subsample',  # Mieux que 'balanced'
            verbose=0,
            max_samples=0.8  # Sous-√©chantillonnage pour √©viter overfitting
        ),
        'description': 'Ensemble d\'arbres - Robuste et performant',
        'color': '#2ecc71'
    },
    
    'XGBoost': {
        'model': XGBClassifier(
            n_estimators=100,  # R√©duit de 150
            max_depth=6,       # R√©duit de 10 √† 6 (XGBoost recommande 3-10)
            learning_rate=0.05, # R√©duit de 0.1 √† 0.05 (apprentissage plus lent)
            subsample=0.8,
            colsample_bytree=0.8,
            gamma=0.1,
            min_child_weight=3,  # Ajout√© pour √©viter overfitting
            scale_pos_weight=min(scale_pos_weight, 10),  # Limit√© √† max 10
            random_state=42,
            n_jobs=-1,
            eval_metric='logloss',
            verbosity=0,
            reg_alpha=0.1,  # R√©gularisation L1
            reg_lambda=1.0  # R√©gularisation L2
        ),
        'description': 'Gradient Boosting optimis√© - Tr√®s haute performance',
        'color': '#e74c3c'
    },
    
    'LightGBM': {
        'model': LGBMClassifier(
            n_estimators=100,
            max_depth=6,      # R√©duit de 10
            learning_rate=0.05, # R√©duit de 0.1
            subsample=0.8,
            colsample_bytree=0.8,
            num_leaves=31,
            min_child_samples=30,  # Augment√© de 20 √† 30
            min_child_weight=0.001,
            random_state=42,
            n_jobs=-1,
            verbose=-1,
            is_unbalance=True,  # OK pour LightGBM
            reg_alpha=0.1,   # R√©gularisation L1
            reg_lambda=1.0   # R√©gularisation L2
        ),
        'description': 'Gradient Boosting l√©ger - Rapide et efficace',
        'color': '#f39c12'
    }
}

print(f"\nüìã {len(models_config)} mod√®les configur√©s avec hyperparam√®tres optimis√©s:")
for name, config in models_config.items():
    print(f"   ‚úì {name}: {config['description']}")

print("\n‚öôÔ∏è Ajustements cl√©s pour √©viter les faux positifs:")
print("   ‚Ä¢ Profondeur d'arbres r√©duite (√©vite overfitting)")
print("   ‚Ä¢ Learning rate plus faible (convergence stable)")
print("   ‚Ä¢ R√©gularisation L1/L2 ajout√©e")
print("   ‚Ä¢ Min samples augment√©s (g√©n√©ralisation)")
print("   ‚Ä¢ scale_pos_weight limit√© (√©vite biais extr√™me)")

# =============================================================================
# AJOUT: FONCTION POUR AJUSTER LE SEUIL DE D√âCISION
# =============================================================================

def find_optimal_threshold(y_true, y_pred_proba, metric='f1'):
    """
    Trouve le seuil optimal pour la classification
    au lieu d'utiliser le seuil par d√©faut de 0.5
    
    Args:
        y_true: Vraies √©tiquettes
        y_pred_proba: Probabilit√©s pr√©dites
        metric: 'f1', 'precision', 'recall', ou 'balanced'
    
    Returns:
        float: Seuil optimal
    """
    from sklearn.metrics import precision_recall_curve, f1_score
    
    thresholds = np.arange(0.1, 0.9, 0.01)
    scores = []
    
    for thresh in thresholds:
        y_pred = (y_pred_proba >= thresh).astype(int)
        
        if metric == 'f1':
            score = f1_score(y_true, y_pred)
        elif metric == 'precision':
            score = precision_score(y_true, y_pred, zero_division=0)
        elif metric == 'recall':
            score = recall_score(y_true, y_pred, zero_division=0)
        elif metric == 'balanced':
            # Balance entre precision et recall
            prec = precision_score(y_true, y_pred, zero_division=0)
            rec = recall_score(y_true, y_pred, zero_division=0)
            score = 2 * (prec * rec) / (prec + rec + 1e-10)
        
        scores.append(score)
    
    optimal_idx = np.argmax(scores)
    optimal_threshold = thresholds[optimal_idx]
    
    return optimal_threshold, scores[optimal_idx]

print("\n‚úÖ Configuration termin√©e!")
print("="*80)

# =============================================================================
# BONUS: AFFICHER LES STATISTIQUES DES FEATURES
# =============================================================================

print("\n" + "="*80)
print("üìä STATISTIQUES DES FEATURES (apr√®s scaling)")
print("="*80)

# Cr√©er un DataFrame avec les features scal√©es
X_train_scaled_df = pd.DataFrame(
    X_train_scaled,
    columns=feature_names
)

print("\nüìà Top 5 features avec variance la plus √©lev√©e:")
variances = X_train_scaled_df.var().sort_values(ascending=False)
for i, (feature, var) in enumerate(variances.head(5).items(), 1):
    print(f"   {i}. {feature}: {var:.4f}")

print("\nüìâ Statistiques descriptives:")
print(X_train_scaled_df.describe().round(4))

# V√©rifier s'il y a des valeurs extr√™mes
print("\n‚ö†Ô∏è D√©tection de valeurs extr√™mes (|z-score| > 3):")
outliers_per_feature = (np.abs(X_train_scaled_df) > 3).sum()
for feature, count in outliers_per_feature[outliers_per_feature > 0].items():
    percentage = (count / len(X_train_scaled_df)) * 100
    print(f"   ‚Ä¢ {feature}: {count:,} outliers ({percentage:.2f}%)")

print("="*80)


# In[144]:


# =============================================================================
# ENTRA√éNEMENT DES MOD√àLES AVEC AJUSTEMENT DU SEUIL
# =============================================================================
from datetime import datetime
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve

print("\n" + "="*80)
print("üéØ ENTRA√éNEMENT DES MOD√àLES AVEC OPTIMISATION DU SEUIL")
print("="*80)

# Dictionnaire pour stocker les r√©sultats
results = {}
optimal_thresholds = {}

for model_name, config in models_config.items():
    print(f"\n{'='*80}")
    print(f"üöÄ Entra√Ænement: {model_name}")
    print(f"   {config['description']}")
    print(f"{'='*80}")
    
    # 1. ENTRA√éNEMENT
    print(f"\n‚è≥ Entra√Ænement en cours...")
    start_time = datetime.now()
    
    model = config['model']
    model.fit(X_train_scaled, y_train)
    
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"‚úÖ Entra√Ænement termin√© en {training_time:.2f}s")
    
    # 2. PR√âDICTIONS SUR VALIDATION
    print(f"\nüìä Pr√©dictions sur validation set...")
    y_val_proba = model.predict_proba(X_val_scaled)[:, 1]
    
    # 3. TROUVER LE SEUIL OPTIMAL
    print(f"\nüéØ Recherche du seuil optimal...")
    
    # Tester diff√©rents seuils
    thresholds_to_test = np.arange(0.1, 0.9, 0.01)
    f1_scores = []
    
    for thresh in thresholds_to_test:
        y_pred_thresh = (y_val_proba >= thresh).astype(int)
        f1 = f1_score(y_val, y_pred_thresh)
        f1_scores.append(f1)
    
    # Trouver le meilleur seuil
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds_to_test[optimal_idx]
    optimal_f1 = f1_scores[optimal_idx]
    
    print(f"   ‚úÖ Seuil optimal trouv√©: {optimal_threshold:.3f}")
    print(f"   ‚úÖ F1-Score optimal: {optimal_f1:.4f}")
    
    optimal_thresholds[model_name] = optimal_threshold
    
    # 4. PR√âDICTIONS AVEC SEUIL OPTIMAL
    y_val_pred_optimal = (y_val_proba >= optimal_threshold).astype(int)
    
    # 5. PR√âDICTIONS AVEC SEUIL PAR D√âFAUT (0.5) pour comparaison
    y_val_pred_default = (y_val_proba >= 0.5).astype(int)
    
    # 6. M√âTRIQUES AVEC SEUIL OPTIMAL
    print(f"\nüìà M√©triques avec seuil optimal ({optimal_threshold:.3f}):")
    
    accuracy_optimal = accuracy_score(y_val, y_val_pred_optimal)
    precision_optimal = precision_score(y_val, y_val_pred_optimal)
    recall_optimal = recall_score(y_val, y_val_pred_optimal)
    f1_optimal = f1_score(y_val, y_val_pred_optimal)
    roc_auc_optimal = roc_auc_score(y_val, y_val_proba)
    
    print(f"   ‚Ä¢ Accuracy:  {accuracy_optimal:.4f}")
    print(f"   ‚Ä¢ Precision: {precision_optimal:.4f}")
    print(f"   ‚Ä¢ Recall:    {recall_optimal:.4f}")
    print(f"   ‚Ä¢ F1-Score:  {f1_optimal:.4f}")
    print(f"   ‚Ä¢ ROC-AUC:   {roc_auc_optimal:.4f}")
    
    # 7. COMPARAISON AVEC SEUIL PAR D√âFAUT
    print(f"\nüìä Comparaison avec seuil par d√©faut (0.5):")
    
    accuracy_default = accuracy_score(y_val, y_val_pred_default)
    precision_default = precision_score(y_val, y_val_pred_default)
    recall_default = recall_score(y_val, y_val_pred_default)
    f1_default = f1_score(y_val, y_val_pred_default)
    
    print(f"   ‚Ä¢ Accuracy:  {accuracy_default:.4f} (Œî {accuracy_optimal - accuracy_default:+.4f})")
    print(f"   ‚Ä¢ Precision: {precision_default:.4f} (Œî {precision_optimal - precision_default:+.4f})")
    print(f"   ‚Ä¢ Recall:    {recall_default:.4f} (Œî {recall_optimal - recall_default:+.4f})")
    print(f"   ‚Ä¢ F1-Score:  {f1_default:.4f} (Œî {f1_optimal - f1_default:+.4f})")
    
    # 8. MATRICE DE CONFUSION
    print(f"\nüìä Matrice de confusion (seuil optimal):")
    cm = confusion_matrix(y_val, y_val_pred_optimal)
    print(f"   TN: {cm[0,0]:,}  |  FP: {cm[0,1]:,}")
    print(f"   FN: {cm[1,0]:,}  |  TP: {cm[1,1]:,}")
    
    # Calculer les taux
    tn, fp, fn, tp = cm.ravel()
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
    
    print(f"\n   ‚Ä¢ Taux de faux positifs (FPR): {fpr:.4f} ({fpr*100:.2f}%)")
    print(f"   ‚Ä¢ Taux de faux n√©gatifs (FNR): {fnr:.4f} ({fnr*100:.2f}%)")
    
    # 9. SAUVEGARDER LES R√âSULTATS
    results[model_name] = {
        'model': model,
        'optimal_threshold': optimal_threshold,
        'metrics_optimal': {
            'accuracy': accuracy_optimal,
            'precision': precision_optimal,
            'recall': recall_optimal,
            'f1_score': f1_optimal,
            'roc_auc': roc_auc_optimal
        },
        'metrics_default': {
            'accuracy': accuracy_default,
            'precision': precision_default,
            'recall': recall_default,
            'f1_score': f1_default
        },
        'confusion_matrix': cm,
        'training_time': training_time,
        'y_val_proba': y_val_proba
    }

# =============================================================================
# COMPARAISON FINALE DES MOD√àLES
# =============================================================================

print("\n\n" + "="*80)
print("üèÜ COMPARAISON FINALE DES MOD√àLES")
print("="*80)

comparison_df = pd.DataFrame({
    'Mod√®le': list(results.keys()),
    'Seuil Optimal': [results[m]['optimal_threshold'] for m in results.keys()],
    'Accuracy': [results[m]['metrics_optimal']['accuracy'] for m in results.keys()],
    'Precision': [results[m]['metrics_optimal']['precision'] for m in results.keys()],
    'Recall': [results[m]['metrics_optimal']['recall'] for m in results.keys()],
    'F1-Score': [results[m]['metrics_optimal']['f1_score'] for m in results.keys()],
    'ROC-AUC': [results[m]['metrics_optimal']['roc_auc'] for m in results.keys()],
    'Temps (s)': [results[m]['training_time'] for m in results.keys()]
})

print("\nüìä Tableau comparatif:")
print(comparison_df.to_string(index=False))

# Trouver le meilleur mod√®le
best_model_name = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Mod√®le']
print(f"\nüèÜ Meilleur mod√®le: {best_model_name}")
print(f"   ‚Ä¢ F1-Score: {results[best_model_name]['metrics_optimal']['f1_score']:.4f}")
print(f"   ‚Ä¢ Seuil optimal: {results[best_model_name]['optimal_threshold']:.3f}")

# =============================================================================
# SAUVEGARDER LE MEILLEUR MOD√àLE AVEC SON SEUIL
# =============================================================================

print("\n" + "="*80)
print("üíæ SAUVEGARDE DU MEILLEUR MOD√àLE")
print("="*80)

best_model = results[best_model_name]['model']
best_threshold = results[best_model_name]['optimal_threshold']

# Sauvegarder le mod√®le
joblib.dump(best_model, 'outputs/best_model.pkl')
print(f"‚úÖ Mod√®le sauvegard√©: outputs/best_model.pkl")

# Sauvegarder le scaler
joblib.dump(scaler, 'outputs/scaler.pkl')
print(f"‚úÖ Scaler sauvegard√©: outputs/scaler.pkl")

# Sauvegarder les m√©tadonn√©es AVEC LE SEUIL OPTIMAL
metadata = {
    'best_model': best_model_name,
    'optimal_threshold': best_threshold,  # ‚Üê IMPORTANT!
    'feature_names': list(feature_names),
    'all_models': {}
}

for model_name in results.keys():
    metadata['all_models'][model_name] = {
        'optimal_threshold': results[model_name]['optimal_threshold'],
        'metrics': results[model_name]['metrics_optimal']
    }

with open('outputs/metadata.json', 'w') as f:
    json.dump(metadata, f, indent=4)

print(f"‚úÖ M√©tadonn√©es sauvegard√©es: outputs/metadata.json")
print(f"\n‚ö° IMPORTANT: Seuil optimal = {best_threshold:.3f} (pas 0.5!)")

print("\n" + "="*80)
print("‚úÖ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!")
print("="*80)


# In[86]:


print("\n" + "="*80)
print("üöÄ ENTRA√éNEMENT DES MOD√àLES (VERSION CORRIG√âE)")
print("="*80)

import warnings
warnings.filterwarnings('ignore')

# Dictionnaire pour stocker les r√©sultats
all_results = {}

# Boucle d'entra√Ænement
for model_name, config in models_config.items():
    
    print(f"\n{'='*80}")
    print(f"üîµ Entra√Ænement: {model_name}")
    print(f"   {config['description']}")
    print(f"{'='*80}")
    
    # D√©marrer un run MLflow
    with mlflow.start_run(run_name=model_name) as run:
        
        # Obtenir le mod√®le
        model = config['model']
        
        # ‚è±Ô∏è Entra√Ænement
        print("‚è≥ Entra√Ænement en cours...")
        start_time = datetime.now()
        
        model.fit(X_train_scaled, y_train)
        
        training_time = (datetime.now() - start_time).total_seconds()
        print(f"‚úÖ Entra√Ænement termin√© en {training_time:.2f} secondes")
        
        # üìä √âvaluation
        print("\nüìä √âvaluation du mod√®le...")
        metrics, viz_figure = evaluate_model_comprehensive(
            model, X_test_scaled, y_test, model_name
        )
        plt.show()
        # Ajouter le temps d'entra√Ænement
        metrics['training_time_seconds'] = training_time
        
        # üìù Logging vers MLflow
        print("\nüìù Logging vers MLflow...")
        
        # 1. Param√®tres
        model_params = {
            'model_type': model_name,
            'algorithm': model.__class__.__name__,
            'train_samples': len(X_train_scaled),
            'test_samples': len(X_test_scaled),
            'n_features': X_train_scaled.shape[1],
            'training_time': training_time
        }
        
        # Ajouter les hyperparam√®tres (filtrer les non-JSON serializable)
        if hasattr(model, 'get_params'):
            hyperparams = model.get_params()
            for key, value in hyperparams.items():
                if isinstance(value, (int, float, str, bool, type(None))):
                    model_params[f'hp_{key}'] = value
        
        mlflow.log_params(model_params)
        
        # 2. M√©triques (filtrer pour ne garder que les nombres)
        numeric_metrics = {
            k: float(v) for k, v in metrics.items() 
            if isinstance(v, (int, float, np.integer, np.floating))
        }
        mlflow.log_metrics(numeric_metrics)
        
        # 3. Visualisations
        mlflow.log_figure(viz_figure, f"{model_name}_evaluation.png")
        plt.close(viz_figure)
        
        # 4. Classification Report
        report = classification_report(
            y_test, model.predict(X_test_scaled),
            target_names=['Non-Fraude', 'Fraude']
        )
        with open(f"{model_name}_report.txt", "w") as f:
            f.write(report)
        mlflow.log_artifact(f"{model_name}_report.txt")
        os.remove(f"{model_name}_report.txt")
        
        # 5. Sauvegarder le mod√®le (SANS registered_model_name pour √©viter l'erreur)
        try:
            mlflow.sklearn.log_model(
                model, 
                "model"
                # PAS de registered_model_name ici
            )
            print("‚úÖ Mod√®le sauvegard√© dans MLflow")
        except Exception as e:
            print(f"‚ö†Ô∏è Avertissement MLflow: {str(e)}")
            print("   Le mod√®le sera enregistr√© manuellement")
        
        # 6. Tags pour organisation
        mlflow.set_tags({
            'project': 'CDDA-Fraud-Detection',
            'framework': 'scikit-learn',
            'use_case': 'fraud_detection',
            'data_balancing': 'SMOTE',
            'deployment_ready': 'true'
        })
        
        print(f"‚úÖ Run ID: {run.info.run_id}")
        
        # Stocker les r√©sultats
        all_results[model_name] = {
            'model': model,
            'metrics': metrics,
            'run_id': run.info.run_id,
            'color': config['color']
        }
    
    print(f"\n{'='*80}")


# In[130]:


scaled_data = scaler.transform(input_data)


# In[90]:


# =============================================================================
# ENREGISTREMENT MANUEL DU MEILLEUR MOD√àLE (apr√®s comparaison)
# =============================================================================

print("\n" + "="*80)
print("üíæ ENREGISTREMENT MANUEL DES MOD√àLES DANS AZURE ML")
print("="*80)

# Fonction pour enregistrer un mod√®le manuellement
def register_model_manually(model, model_name, metrics, run_id):
    """
    Enregistre un mod√®le dans Azure ML Model Registry de mani√®re manuelle
    """
    try:
        from azureml.core import Model
        
        # Cr√©er un dossier temporaire
        temp_dir = f'temp_model_{model_name}'
        os.makedirs(temp_dir, exist_ok=True)
        
        # Sauvegarder le mod√®le localement
        model_path = os.path.join(temp_dir, 'model.pkl')
        joblib.dump(model, model_path)
        
        # Enregistrer dans Azure ML
        registered_model = Model.register(
            workspace=ws,
            model_path=temp_dir,
            model_name=f'fraud_detection_{model_name.lower()}',
            description=f'{model_name} - F1:{metrics["f1_score"]:.4f} ROC-AUC:{metrics["roc_auc"]:.4f}',
            tags={
                'model_type': model_name,
                'f1_score': f'{metrics["f1_score"]:.4f}',
                'roc_auc': f'{metrics["roc_auc"]:.4f}',
                'accuracy': f'{metrics["accuracy"]:.4f}',
                'mlflow_run_id': run_id,
                'deployment_ready': 'true'
            },
            model_framework='ScikitLearn'
        )
        
        # Nettoyer
        import shutil
        shutil.rmtree(temp_dir)
        
        print(f"‚úÖ {model_name} enregistr√©:")
        print(f"   Nom: {registered_model.name}")
        print(f"   Version: {registered_model.version}")
        print(f"   ID: {registered_model.id}")
        
        return registered_model
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'enregistrement de {model_name}: {str(e)}")
        return None

# Enregistrer tous les mod√®les
if AZURE_ML_CONNECTED:
    print("\nüìù Enregistrement de tous les mod√®les dans Azure ML...")
    registered_models = {}
    
    for model_name, results in all_results.items():
        registered_model = register_model_manually(
            model=results['model'],
            model_name=model_name,
            metrics=results['metrics'],
            run_id=results['run_id']
        )
        if registered_model:
            registered_models[model_name] = registered_model
    
    print(f"\n‚úÖ {len(registered_models)} mod√®les enregistr√©s avec succ√®s!")
else:
    print("\n‚ö†Ô∏è Mode local - enregistrement Azure ML d√©sactiv√©")


# In[68]:


# =============================================================================
# SAUVEGARDER LE MEILLEUR MOD√àLE LOCALEMENT POUR L'ENREGISTREMENT
# =============================================================================

print("\n" + "="*80)
print("üèÜ IDENTIFICATION ET SAUVEGARDE DU MEILLEUR MOD√àLE")
print("="*80)

# 1. Trouver le meilleur mod√®le bas√© sur F1-Score ou ROC-AUC
best_model_name = None
best_score = 0
best_metric = 'f1_score'  # ou 'roc_auc' selon votre pr√©f√©rence

print(f"\nüìä Comparaison des mod√®les (crit√®re: {best_metric}):")
for model_name, results in all_results.items():
    score = results['metrics'][best_metric]
    print(f"   {model_name}: {score:.4f}")
    
    if score > best_score:
        best_score = score
        best_model_name = model_name

print(f"\nü•á Meilleur mod√®le: {best_model_name} ({best_metric}={best_score:.4f})")

# 2. R√©cup√©rer le meilleur mod√®le
best_model = all_results[best_model_name]['model']
best_metrics = all_results[best_model_name]['metrics']

# 3. Cr√©er le dossier outputs
os.makedirs('outputs', exist_ok=True)

# 4. Sauvegarder le meilleur mod√®le
print("\nüíæ Sauvegarde des artifacts dans /outputs...")

# Sauvegarder le mod√®le
with open('outputs/best_model.pkl', 'wb') as f:
    joblib.dump(best_model, f)
print("   ‚úÖ best_model.pkl")

# Sauvegarder le scaler (si vous l'avez utilis√©)
if 'scaler' in globals():
    with open('outputs/scaler.pkl', 'wb') as f:
        joblib.dump(scaler, f)
    print("   ‚úÖ scaler.pkl")
else:
    # Cr√©er un scaler dummy si n√©cessaire
    from sklearn.preprocessing import StandardScaler
    dummy_scaler = StandardScaler()
    with open('outputs/scaler.pkl', 'wb') as f:
        joblib.dump(dummy_scaler, f)
    print("   ‚ö†Ô∏è  scaler.pkl (dummy - remplacez par le vrai scaler)")

# Sauvegarder les m√©tadonn√©es
metadata = {
    'best_model_name': best_model_name,
    'best_model_type': type(best_model).__name__,
    'metrics': {k: float(v) for k, v in best_metrics.items()},
    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'all_models_comparison': {
        name: {
            'f1_score': float(res['metrics']['f1_score']),
            'roc_auc': float(res['metrics']['roc_auc']),
            'accuracy': float(res['metrics']['accuracy'])
        }
        for name, res in all_results.items()
    },
    'feature_names': X_test.columns.tolist() if hasattr(X_test, 'columns') else [],
    'n_features': X_test.shape[1] if hasattr(X_test, 'shape') else 0
}

with open('outputs/metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
print("   ‚úÖ metadata.json")

# 5. V√©rification
print("\nüîç V√©rification des fichiers:")
for filename in ['best_model.pkl', 'scaler.pkl', 'metadata.json']:
    filepath = f'outputs/{filename}'
    if os.path.exists(filepath):
        size = os.path.getsize(filepath) / 1024  # En KB
        print(f"   ‚úÖ {filename} ({size:.2f} KB)")
    else:
        print(f"   ‚ùå {filename} MANQUANT!")

print("\n‚úÖ Artifacts pr√™ts pour l'enregistrement Azure ML!")
print("   Vous pouvez maintenant ex√©cuter le code d'enregistrement.")


# In[145]:


# =============================================================================
# SAUVEGARDE LOCALE (Toujours fonctionnel)
# =============================================================================

print("\n" + "="*80)
print("üíæ SAUVEGARDE LOCALE DES MOD√àLES (Backup)")
print("="*80)

# Cr√©er le dossier outputs
os.makedirs('outputs', exist_ok=True)

# Sauvegarder tous les mod√®les localement
for model_name, results in all_results.items():
    model_path = f'outputs/{model_name.lower().replace(" ", "_")}.pkl'
    joblib.dump(results['model'], model_path)
    print(f"‚úÖ {model_name} ‚Üí {model_path}")

# Sauvegarder le scaler
scaler_path = 'outputs/scaler.pkl'
joblib.dump(scaler, scaler_path)
print(f"‚úÖ Scaler ‚Üí {scaler_path}")

# Sauvegarder les m√©tadonn√©es de tous les mod√®les
all_metadata = {}
for model_name, results in all_results.items():
    all_metadata[model_name] = {
        'metrics': {
            k: float(v) if isinstance(v, (int, float, np.integer, np.floating)) else str(v)
            for k, v in results['metrics'].items()
        },
        'run_id': results['run_id']
    }

with open('outputs/all_models_metadata.json', 'w') as f:
    json.dump(all_metadata, f, indent=4)
print(f"‚úÖ M√©tadonn√©es ‚Üí outputs/all_models_metadata.json")

print("\n‚úÖ Tous les mod√®les sauvegard√©s localement!")


# ## üìä 9. Deploiement

# In[70]:


# =============================================================================
# √âTAPE 1: PR√âPARER LES FICHIERS DE D√âPLOIEMENT
# =============================================================================

print("="*80)
print(" "*20 + "üöÄ D√âPLOIEMENT AZURE ML ENDPOINT")
print("="*80)

import os
from azureml.core import Workspace, Model, Environment
from azureml.core.model import InferenceConfig
from azureml.core.webservice import AciWebservice, Webservice
from azureml.exceptions import WebserviceException
import json

# Connexion au workspace
try:
    ws = Workspace.from_config()
    print(f"‚úÖ Connect√© au workspace: {ws.name}")
except Exception as e:
    print(f"‚ùå Erreur de connexion: {e}")
    print("üí° Assurez-vous que config.json existe")
    raise


# In[71]:


# =============================================================================
# √âTAPE 2: CR√âER LE SCRIPT DE SCORING
# =============================================================================

print("\nüìù Cr√©ation du script de scoring (score.py)...")

score_script = """
import json
import joblib
import numpy as np
import pandas as pd
from azureml.core.model import Model

def init():
    '''
    Cette fonction est appel√©e une seule fois au d√©marrage du service.
    Elle charge le mod√®le et le scaler en m√©moire.
    '''
    global model, scaler, feature_names
    
    try:
        # Charger le mod√®le
        model_path = Model.get_model_path('fraud-detection-model')
        model = joblib.load(model_path)
        print(f"‚úÖ Mod√®le charg√©: {type(model).__name__}")
        
        # Charger le scaler
        scaler_path = Model.get_model_path('fraud-detection-scaler')
        scaler = joblib.load(scaler_path)
        print(f"‚úÖ Scaler charg√©: {type(scaler).__name__}")
        
        # Charger les noms de features (optionnel)
        try:
            metadata_path = Model.get_model_path('fraud-detection-metadata')
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
                feature_names = metadata.get('feature_names', [])
                print(f"‚úÖ M√©tadonn√©es charg√©es ({len(feature_names)} features)")
        except:
            feature_names = []
            print("‚ö†Ô∏è M√©tadonn√©es non disponibles")
        
        print("‚úÖ Initialisation r√©ussie!")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'initialisation: {str(e)}")
        raise

def run(raw_data):
    '''
    Cette fonction est appel√©e pour chaque requ√™te HTTP.
    Elle re√ßoit les donn√©es, fait la pr√©diction et retourne le r√©sultat.
    
    Format d'entr√©e attendu:
    {
        "data": [[feature1, feature2, ..., featureN]],
        "transaction_ids": ["TXN_001", "TXN_002"]  # optionnel
    }
    
    Format de sortie:
    {
        "predictions": [
            {
                "transaction_id": "TXN_001",
                "is_fraud": true/false,
                "fraud_probability": 0.85,
                "confidence": 0.85,
                "risk_level": "HIGH/MEDIUM/LOW"
            }
        ],
        "model_info": {
            "model_name": "XGBoost",
            "version": "1.0"
        },
        "status": "success"
    }
    '''
    try:
        # Parser les donn√©es JSON
        data = json.loads(raw_data)
        
        # Extraire les features
        if 'data' not in data:
            return json.dumps({
                'error': 'Missing "data" field in request',
                'status': 'error'
            })
        
        input_data = np.array(data['data'])
        transaction_ids = data.get('transaction_ids', 
                                   [f'TXN_{i:04d}' for i in range(len(input_data))])
        
        # Validation de la forme des donn√©es
        if len(input_data.shape) == 1:
            input_data = input_data.reshape(1, -1)
        
        # Preprocessing: Scaling
        scaled_data = scaler.transform(input_data)
        
        # Pr√©diction
        predictions = model.predict(scaled_data)
        probabilities = model.predict_proba(scaled_data)
        
        # Formater les r√©sultats
        results = []
        for i, (pred, proba) in enumerate(zip(predictions, probabilities)):
            fraud_prob = float(proba[1])
            
            # D√©terminer le niveau de risque
            if fraud_prob >= 0.7:
                risk_level = "HIGH"
            elif fraud_prob >= 0.4:
                risk_level = "MEDIUM"
            else:
                risk_level = "LOW"
            
            results.append({
                'transaction_id': transaction_ids[i],
                'is_fraud': bool(pred == 1),
                'fraud_probability': round(fraud_prob, 4),
                'legitimate_probability': round(float(proba[0]), 4),
                'confidence': round(float(max(proba)), 4),
                'risk_level': risk_level,
                'recommendation': (
                    'BLOCK - Fraude probable' if fraud_prob >= 0.7 else
                    'REVIEW - Investigation recommand√©e' if fraud_prob >= 0.4 else
                    'APPROVE - Transaction s√ªre'
                )
            })
        
        # R√©ponse compl√®te
        response = {
            'predictions': results,
            'model_info': {
                'model_name': type(model).__name__,
                'version': '1.0',
                'features_count': input_data.shape[1]
            },
            'metadata': {
                'total_transactions': len(results),
                'fraud_detected': sum(1 for r in results if r['is_fraud']),
                'avg_fraud_probability': round(
                    sum(r['fraud_probability'] for r in results) / len(results), 4
                )
            },
            'status': 'success'
        }
        
        return json.dumps(response)
        
    except Exception as e:
        error_response = {
            'error': str(e),
            'error_type': type(e).__name__,
            'status': 'error',
            'message': 'Une erreur est survenue lors de la pr√©diction'
        }
        return json.dumps(error_response)
"""

# Sauvegarder le script
with open('score.py', 'w', encoding='utf-8') as f:
    f.write(score_script)

print("‚úÖ score.py cr√©√© avec succ√®s")


# In[72]:


# =============================================================================
# √âTAPE 3: CR√âER LE FICHIER D'ENVIRONNEMENT
# =============================================================================

print("\nüì¶ Cr√©ation de l'environnement (fraud_env.yml)...")

env_yml = """name: fraud-detection-inference-env
channels:
  - conda-forge
dependencies:
  - python=3.10.18
  - pip
  - pip:
      - azureml-defaults==1.60.0
      - pandas==1.5.3
      - numpy==1.22.4
      - scikit-learn==1.7.2
      - joblib==1.2.0

"""

with open('fraud_env.yml', 'w') as f:
    f.write(env_yml)

print("‚úÖ fraud_env.yml cr√©√©")


# In[129]:


# =============================================================================
# √âTAPE 4: ENREGISTRER LES MOD√àLES DANS AZURE ML
# =============================================================================

print("\n" + "="*80)
print("üìù ENREGISTREMENT DES MOD√àLES DANS AZURE ML MODEL REGISTRY")
print("="*80)

def register_model_file(workspace, model_path, model_name, description, tags=None):
    """Enregistre un mod√®le dans Azure ML"""
    try:
        # V√©rifier si le fichier existe
        if not os.path.exists(model_path):
            print(f"‚ùå Fichier introuvable: {model_path}")
            return None
        
        # Enregistrer
        model = Model.register(
            workspace=workspace,
            model_path=model_path,
            model_name=model_name,
            description=description,
            tags=tags or {},
            model_framework='ScikitLearn'
        )
        
        print(f"‚úÖ {model_name}")
        print(f"   Version: {model.version}")
        print(f"   ID: {model.id}")
        return model
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'enregistrement de {model_name}: {str(e)}")
        return None

# Enregistrer le meilleur mod√®le
print("\n1Ô∏è‚É£ Enregistrement du mod√®le principal...")
model_registered = register_model_file(
    workspace=ws,
    model_path='outputs/best_model.pkl',
    model_name='fraud-detection-model',
    description='Meilleur mod√®le de d√©tection de fraude (XGBoost/LightGBM)',
    tags={
        'type': 'fraud_detection',
        'framework': 'scikit-learn',
        'deployment': 'production'
    }
)

# Enregistrer le scaler
print("\n2Ô∏è‚É£ Enregistrement du scaler...")
scaler_registered = register_model_file(
    workspace=ws,
    model_path='outputs/scaler.pkl',
    model_name='fraud-detection-scaler',
    description='RobustScaler pour preprocessing des features',
    tags={'type': 'preprocessing'}
)

# Enregistrer les m√©tadonn√©es (optionnel)
print("\n3Ô∏è‚É£ Enregistrement des m√©tadonn√©es...")
if os.path.exists('outputs/metadata.json'):
    metadata_registered = register_model_file(
        workspace=ws,
        model_path='outputs/metadata.json',
        model_name='fraud-detection-metadata',
        description='M√©tadonn√©es du mod√®le (features, m√©triques)',
        tags={'type': 'metadata'}
    )

if not model_registered or not scaler_registered:
    print("\n‚ùå Enregistrement des mod√®les √©chou√©. V√©rifiez les fichiers dans /outputs")
    raise Exception("Mod√®les non enregistr√©s")

print("\n‚úÖ Tous les artifacts enregistr√©s avec succ√®s!")


# In[52]:


# =============================================================================
# √âTAPE 5: CR√âER L'ENVIRONNEMENT AZURE ML
# =============================================================================

print("\n" + "="*80)
print("üåç CR√âATION DE L'ENVIRONNEMENT AZURE ML")
print("="*80)

# Cr√©er l'environnement depuis le fichier YAML
env = Environment.from_conda_specification(
    name='fraud-detection-env',
    file_path='fraud_env.yml'
)

# Enregistrer l'environnement
env.register(workspace=ws)
print(f"‚úÖ Environnement '{env.name}' cr√©√© et enregistr√©")


# In[53]:


# =============================================================================
# √âTAPE 6: CONFIGURATION DU D√âPLOIEMENT
# =============================================================================

print("\n" + "="*80)
print("‚öôÔ∏è CONFIGURATION DU D√âPLOIEMENT")
print("="*80)

# Configuration de l'inf√©rence
inference_config = InferenceConfig(
    entry_script='score.py',
    environment=env
)
print("‚úÖ Configuration d'inf√©rence cr√©√©e")

# Configuration du service (Azure Container Instance)
deployment_config = AciWebservice.deploy_configuration(
    cpu_cores=2,
    memory_gb=4,
    auth_enabled=True,  # Activer l'authentification
    enable_app_insights=True,  # Activer le monitoring
    collect_model_data=True,  # Collecter les donn√©es pour monitoring
    description='API REST de d√©tection de fraude - Projet CDDA',
    tags={
        'project': 'CDDA',
        'use_case': 'fraud_detection',
        'department': 'data_science'
    }
)
print("‚úÖ Configuration de d√©ploiement cr√©√©e (ACI)")
print("   ‚Ä¢ CPU: 2 cores")
print("   ‚Ä¢ RAM: 4 GB")
print("   ‚Ä¢ Auth: Activ√©e")
print("   ‚Ä¢ Monitoring: Activ√©")


# In[25]:


from azureml.core import Workspace
ws.get_details()


# In[57]:


from azure.identity import DefaultAzureCredential
from azure.ai.ml import MLClient

credential = DefaultAzureCredential()  # ensure new token
ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)


# In[56]:


# =============================================================================
# √âTAPE 7: D√âPLOYER LE SERVICE
# =============================================================================

print("\n" + "="*80)
print("üöÄ D√âPLOIEMENT DU SERVICE")
print("="*80)

service_name = 'fraud-detection-api'

# V√©rifier si le service existe d√©j√†
try:
    existing_service = Webservice(workspace=ws, name=service_name)
    print(f"‚ö†Ô∏è Service '{service_name}' existe d√©j√†")
    
    # Demander confirmation pour mise √† jour
    update_service = True  # Mettre √† False si vous voulez garder l'ancien
    
    if update_service:
        print("üîÑ Mise √† jour du service existant...")
        existing_service.delete()
        print("‚úÖ Ancien service supprim√©")
    else:
        print("‚ÑπÔ∏è Utilisation du service existant")
        service = existing_service
        
except WebserviceException:
    print(f"‚ÑπÔ∏è Aucun service existant nomm√© '{service_name}'")

# D√©ployer le nouveau service
if 'service' not in locals():
    print(f"\nüöÄ D√©ploiement en cours de '{service_name}'...")
    print("‚è≥ Cela peut prendre 5-10 minutes...")
    
    service = Model.deploy(
        workspace=ws,
        name=service_name,
        models=[model_registered, scaler_registered],
        inference_config=inference_config,
        deployment_config=deployment_config,
        overwrite=True
    )
    
    # Attendre la fin du d√©ploiement
    service.wait_for_deployment(show_output=True)


# In[62]:


# =============================================================================
# √âTAPE 8: V√âRIFIER LE D√âPLOIEMENT
# =============================================================================

print("\n" + "="*80)
print("‚úÖ D√âPLOIEMENT R√âUSSI!")
print("="*80)

print(f"\nüìä Informations du service:")
print(f"   Nom: {service.name}")
print(f"   √âtat: {service.state}")
print(f"   Scoring URI: {service.scoring_uri}")

# R√©cup√©rer les cl√©s d'authentification
try:
    primary_key, secondary_key = service.get_keys()
    print(f"\nüîë Cl√©s d'authentification:")
    print(f"   Primary Key: {primary_key[:20]}...")
    print(f"   Secondary Key: {secondary_key[:20]}...")
except:
    print("\n‚ö†Ô∏è Authentification par cl√© non disponible")

# Swagger URI (documentation auto-g√©n√©r√©e)
if hasattr(service, 'swagger_uri') and service.swagger_uri:
    print(f"\nüìñ Documentation Swagger: {service.swagger_uri}")

# Sauvegarder les informations de d√©ploiement
deployment_info = {
    'service_name': service.name,
    'scoring_uri': service.scoring_uri,
    'state': service.state,
    'primary_key': primary_key if 'primary_key' in locals() else None,
    'deployment_date': str(pd.Timestamp.now())
}

with open('outputs/deployment_info.json', 'w') as f:
    json.dump(deployment_info, f, indent=4)

print("\nüíæ Informations sauvegard√©es dans: outputs/deployment_info.json")


# In[63]:


# =============================================================================
# √âTAPE 9: TESTER L'ENDPOINT
# =============================================================================

print("\n" + "="*80)
print("üß™ TEST DE L'ENDPOINT")
print("="*80)

import requests
import pandas as pd

# Pr√©parer des donn√©es de test
print("\nüìä Pr√©paration des donn√©es de test...")

# Charger quelques transactions de test
# (Remplacez par vos vraies donn√©es)
test_data = {
    'data': [
        # Transaction 1: Caract√©ristiques normales
        [100.0, 5000.0, 3000.0, 0.5, 0.2],
        # Transaction 2: Caract√©ristiques suspectes
        [50000.0, 100.0, 200000.0, 0.9, 0.8],
        # Transaction 3: Borderline
        [1000.0, 2000.0, 2500.0, 0.45, 0.35]
    ],
    'transaction_ids': ['TXN_001', 'TXN_002', 'TXN_003']
}

print(f"‚úÖ {len(test_data['data'])} transactions de test pr√©par√©es")

# Faire la requ√™te
print("\nüåê Envoi de la requ√™te √† l'endpoint...")

headers = {
    'Content-Type': 'application/json',
    'Authorization': f'Bearer {primary_key}' if 'primary_key' in locals() else ''
}

try:
    response = requests.post(
        service.scoring_uri,
        data=json.dumps(test_data),
        headers=headers,
        timeout=30
    )
    
    if response.status_code == 200:
        results = response.json()
        
        print("\n‚úÖ PR√âDICTIONS RE√áUES:")
        print("="*70)
        
        for pred in results['predictions']:
            status = "üö® FRAUDE" if pred['is_fraud'] else "‚úÖ L√âGITIME"
            risk = pred['risk_level']
            prob = pred['fraud_probability'] * 100
            
            print(f"\n{pred['transaction_id']}: {status}")
            print(f"   Probabilit√© fraude: {prob:.1f}%")
            print(f"   Niveau de risque: {risk}")
            print(f"   Recommandation: {pred['recommendation']}")
        
        print("\n" + "="*70)
        print(f"üìä Statistiques globales:")
        print(f"   Total: {results['metadata']['total_transactions']}")
        print(f"   Fraudes d√©tect√©es: {results['metadata']['fraud_detected']}")
        print(f"   Prob. moyenne: {results['metadata']['avg_fraud_probability']*100:.1f}%")
        
    else:
        print(f"‚ùå Erreur HTTP {response.status_code}")
        print(f"R√©ponse: {response.text}")
        
except Exception as e:
    print(f"‚ùå Erreur lors du test: {str(e)}")


# In[24]:


import sys
print(sys.version)


# In[94]:


correlations = pd.concat([X_train, y_train], axis=1).corr()
correlations['isFraud'].abs().sort_values(ascending=False).head(10)

